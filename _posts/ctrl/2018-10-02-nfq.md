---
layout: post
title: "Current Topics in Reinforcement Learning: Neural Fitted Q Iteration (Riedmiller, 2005)"
author: Seungjae Ryan Lee
permalink: /ctrl/neural-fitted-q-iteration/
redirect-from: /ctrl/nfq/
published: false

front_image: /assets/blog/ctrl/nfq/front.png
front_image_type: contain
front_text: >
    This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based RL algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are neeed to generate control policies of high quality.
---

![Abstract]({{ "/assets/blog/ctrl/nfq/abstract.png" | absolute_url }})

**Title**: Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method<br/>
**Author**: Martin Riedmiller<br/>
**Affiliation**: Professor at [University of Onsabruck](https://www.uni-osnabrueck.de/en/home.html)<br/>
**Current Affiliation**: Research Scientist at [DeepMind](https://deepmind.com)

**Prerequisites**
 - *Reinforcement Learning: An Introduction 2nd Edition* Part 1 (Sutton and Barto, 2018) [[PDF]](https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view)

**Accompanying Resources**
 - 

<hr/>

## 1  Introduction

To handle real-world problems with massive scale, reinforcement Learning must be accompanied with powerful function approximation methods. Unfortunately, Boyan and Moore (1995) showed that using function approximators leads to poor convergence properties even in simple problems.

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/BM95.png" alt="BM95"/>
  <figcaption>From <em>Generalization in reinforcement learning: Safely approximating the value function</em> (Boyan and Moore, 1995)</figcaption>
</figure>


## 2  Main Idea

### 2.1  Markovian Decision Processes (MDP)
### 2.2  Classical Q-Learning
### 2.3  Q-Learning for Neural Networks

## 3  Neural Fitted Q Iteration (NFQ)

### 3.1  Basic Idea
### 3.2  The NFQ - Algorithm
### 3.3  Sample setting of Costs
### 3.4  Variants

## 4  Benchmarking

### 4.1  Type of Tasks
### 4.2  Evaluating Learning Performance
### 4.3  Evaluating Controller Performance

## 5 Empirical Results

### 5.1  The Pole Balancing Task
### 5.2  The Mountain Car Benchmark
### 5.3  The CartPole Regulator Benchmark

## 6 Conclusion

<hr/>

## Final Thoughts


**Recommended Next Papers**
 - Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013) [[Arxiv]](https://arxiv.org/abs/1312.5602)

---
layout: post
title: "Current Topics in Reinforcement Learning: Neural Fitted Q Iteration (Riedmiller, 2005)"
author: Seungjae Ryan Lee
permalink: /ctrl/neural-fitted-q-iteration/
redirect-from: /ctrl/nfq/
# published: false

front_image: /assets/blog/ctrl/nfq/front.png
front_image_type: contain
front_text: >
    This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based RL algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are neeed to generate control policies of high quality.

nav:
- name: 1  Introduction
  permalink: '#1-introduction'
- name: 2  Main Idea
  permalink: '#2-main-idea'
- name: 3  Neural Fitted Q Iteration (NFQ)
  permalink: '#3-neural-fitted-q-iteration-nfq'
- name: 4  Benchmarking
  permalink: '#4-benchmarking'
- name: 5  Empirical Results
  permalink: '#5-empirical-results'
- name: 6  Conclusion
  permalink: '#6-conclusion'
- name: Final Thoughts
  permalink: '#final-thoughts'
---

![Abstract]({{ "/assets/blog/ctrl/nfq/abstract.png" | absolute_url }})

**Title**: Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method<br/>
**Author**: Martin Riedmiller<br/>
**Affiliation**: Professor at [University of Onsabruck](https://www.uni-osnabrueck.de/en/home.html)<br/>
**Current Affiliation**: Research Scientist at [DeepMind](https://deepmind.com)

**Prerequisites**
 - *Reinforcement Learning: An Introduction 2nd Edition* Part 1 (Sutton and Barto, 2018) [[PDF]](https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view)

**Accompanying Resources**
 - *Generalization in reinforcement learning: Safely approximating the value function* (Boyan and Moore, 1995) [[PDF]](https://www.ri.cmu.edu/pub_files/pub1/boyan_justin_1995_1/boyan_justin_1995_1.pdf)
 - *Self-improving reactive agents based on reinforcement learning, planning and teaching* (Lin, 1992) [[PDF]](http://www.incompleteideas.net/lin-92.pdf)
 - *Tree-Based Batch Mode Reinforcement Learning* (Ernst et al., 2005) [[PDF]](http://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf)
 - *Least-Squares Policy Iteration* (Lagoudakis and Parr, 2003) [[PDF]](http://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.pdf)

<hr/>

## 1 Introduction

To handle real-world problems with massive scale, reinforcement learning must be accompanied with powerful function approximation methods. However, function approximation is a double-edged sword: its generalization effects can accelerate learning, but weight changes affect unrelated regions, making the agent forget knowledge in regions less frequently visited. Boyan and Moore (1995) function approximators lead to poor convergence properties even in simple problems.

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/BM95.png" alt="BM95"/>
  <figcaption>From <em>Generalization in reinforcement learning: Safely approximating the value function</em> (Boyan and Moore, 1995)</figcaption>
</figure>

To prevent the agent from forgetting meaningful knowledge, we propose reusing previous knowledge. We store all previous experiences in the form of state-action transitions. Then, for every update, the entire batch of experiences is reused to update the neural network. This is a special form of **experience replay** proposed by Lin (1992).

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/Lin92.png" alt="Lin92"/>
  <figcaption>From <em>Self-improving reactive agents based on reinforcement learning, planning and teaching</em> (Lin, 1992)</figcaption>
</figure>

This algorithm belongs to a family of **fitted value iteration** algorithms, a family of value iteration algorithms paired with function approximation. Various function approximations are possible, including randomized trees by Ernst et al. (2005). This algorithm differs by using a multilayered perceptron (MLP), and is therefore called **Neural Fitted Q Iteration** (NFQ). The paper focuses on the following three properties of NFQ:

 1. NFQ is model-free.
 2. NFQ is data-efficient due to experience replay.
 3. NFQ shows comparable results to analytically designed controllers.


## 2 Main Idea

### 2.1 Markovian Decision Processes (MDP)

We expect the readers to be familiar with **Markov Decision Processes** (MDPs). An MDP can be described by a set of states $S$, set of actions $A$, a stochastic transition function $p(s, a, s')$, and a reward or cost function $c: S \times A \to \mathbb{R}$. Unlike most current reinforcement learning papers that use the reward function $r$, this paper uses the cost function $c$. Thus, the objective of the agent is to find an optimal policy $\pi^* : S \to A$ that minimizes the expected cumulative cost for each state.

Vanilla value iteration can only be used in finite number of states and actions. By using function approximation, we can ease the restriction on the number of states and allow $S$ to be continuous.

### 2.2 Classical Q-Learning

We also expect the readers to have basic knowledge of classical tabular **Q-Learning** algorithm. Note that we use the minimum of all actions since we use the cost function $c$.

$$
Q_{k+1} (s, a) := (1 - \alpha) Q(s, a) + \alpha \left( c(s, a) + \gamma \min_b Q_k (s', b) \right)
$$

where $s$ is the state before the transition, $a$ is the action applied, $s'$ is the state after the action, $\alpha$ is the learning rate, and $\gamma$ is the discount factor. Q-learning is guaranteed to converge to the optimal action value $q^*$ for finite $S$ and $A$ if every state-action pair $(s, a) \in S \times A$ is updated infinitely often. In classical Q-Learning, updates are done online in a sample-by-sample manner.

### 2.3 Q-Learning for Neural Networks

To integrate neural networks into Q-learning, one small change must be made. Because Q-values are no longer stored in a tabular manner but instead encoded in the neural network, we need to compose an error function to update the weights of the neural network through backpropagation. A simple error function would be the squared error of the current Q-value and the target Q-value.

$$
error = \left( Q(s, a) - \left( c(s, a) + \gamma \min_b Q(s', b) \right) \right)^2
$$

However, this naive approach of replacing tabular representation with function approximators results in slow learning. With the online update, update from each state action pair affects other states and actions.

## 3 Neural Fitted Q Iteration (NFQ)

### 3.1 Basic Idea

The central idea of NFQ is to replace online single-sample updates with offline batch updates. Using batch updates allows for advanced supervised learning methods that are faster and more robust. We use Rprop by Riedmiller and Braun (1993), which free us from tuning hyperparamters related to the neural network.

A set of experiences $D$ is collected by interacting with real or simulated environment. Each experience is a transition $(s, a, s')$, where $s$ is the original state, $a$ is the chosen action, and $s'$ is the resulting state. It is also possible to include the reward / cost received. However, we perceive the immediate cost as something specified by the designer instead of something observed by the environment. Thus, we leave out costs to specify it at a later point.

### 3.2 The NFQ - Algorithm

The NFQ algorithm consists of two steps:

 1. Generate the training set $P$ from transition samples $D$. $P$ consists of pairs of input $(s, a)$ and target Q-value $c(s, a, s') + \gamma \min_b Q(s', b)$, generated from transitions $(s, a, s') \in D$.
 2. Train the neural network with the training set $P$. This is repeated for several epochs.

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/figure1.png" alt="Main loop of NFQ"/>
  <figcaption></figcaption>
</figure>

### 3.3 Sample Setting of Costs

As we have mentioned previously, we do not save the cost in the transition samples $D$. Instead, we design the cost function by partitioning the set of all states $S$ to goal states $S^+$, forbidden states $S^-$, and other states. We define the target differently for each set of states:

<figure>
$$
target = 
\begin{cases} 
  c(s, a, s') & \text{if } s \in S^+ \\
  C^- & \text{if } s \in S^- \\
  c(s, a, s') + \gamma \min Q(s', b) & \text{else (standard case)}
\end{cases}
$$
<figcaption><strong>Eqn 1.</strong> General target function</figcaption>
</figure>

If the state ends in goal states $S+$ or forbidden states $S^-$, the episode is terminated. Thus, the target is simply the immediate cost $c(s, a, s')$ or $C^-$. In other states, the costs from the remainder of the episode must also be considered, so $\gamma \min_b Q(s', b)$ must be added.

For simplicity, we use a simplified version where the cost function is constant for all non-forbidden states. This makes the problem a **minimum-time controller problem**, where the task is to reach the goal states as soon as possible.

<figure>
$$
target = 
\begin{cases} 
  c_{trans} & \text{if } s \in S^+ \\
  1 & \text{if } s \in S^- \\
  c_{trans} + \gamma \min Q(s', b) & \text{else (standard case)}
\end{cases}
$$
<figcaption><strong>Eqn 2.</strong> Target for minimum-time controller problems</figcaption>
</figure>

We can modify the target slightly to make it valid for environments where the task is to stay in the goal state as long as possible. These problems are called **regulator problems**. In this case, the immediate cost is 0 at goal state, so the target is just the estimated cumulative cost afterwards.

<figure>
$$
target = 
\begin{cases} 
  \gamma \min Q(s', b) & \text{if } s \in S^+ \\
  1 & \text{if } s \in S^- \\
  c_{trans} + \gamma \min Q(s', b) & \text{else (standard case)}
\end{cases}
$$
<figcaption><strong>Eqn 3.</strong> Target for regulator problems</figcaption>
</figure>

### 3.4 Variants

We discuss two possible variants of the basic NFQ algorithm. The first variant is about how the experiences $D$ is collected. In the basic algorithm, $D$ is collected as "random rollouts": the experiences are from interactions with environment through random play. However, this might result in an incomplete set of experiences to find the optimal policy. Thus, instead of random rollouts, a better method would be to collect training samples using the current $Q_k$ function.

Another variant is to add "artificial" training pattern called the **hint-to-goal heuristic**. Since the goal region is defined as part of the problem, it is possible to generate training data in the goal region. This artificial data is useful since by definition their target value is 0 (except for regulator problems), so training on these synthetic data "clamps" the value function to 0 at the goal region.

## 4 Benchmarking

### 4.1 Type of Tasks

To test the NFQ algorithm, we define three basic control problem:

1. Pole Balancing (a.k.a. Inverted Pendulum): Avoid forbidden states $S^-$
2. Mountain Car: Reach goal states $S^+$
3. CartPole: Stay in the goal states $S^+$

<div class="mdl-grid">
  <div class="mdl-cell mdl-cell--4-col mdl-cell--4-col-desktop mdl-cell--12-col-phone">
    <img src="{{absolute_url}}/assets/blog/ctrl/nfq/pole-balancing.gif" alt="Pole Balancing GIF"/>
  </div>
  <div class="mdl-cell mdl-cell--4-col mdl-cell--4-col-desktopmdl-cell--12-col-phone">
    <img src="{{absolute_url}}/assets/blog/ctrl/nfq/mountain-car.gif" alt="Mountain Car GIF"/>
  </div>
  <div class="mdl-cell mdl-cell--4-col mdl-cell--4-col-desktop mdl-cell--12-col-phone">
    <img src="{{absolute_url}}/assets/blog/ctrl/nfq/cartpole.gif" alt="CartPole GIF"/>
  </div>
</div>



### 4.2 Evaluating Learning Performance

There are two important considerations to learning agents: the speed of learning and the performance. Let's discuss  the speed of learning, or the "learning performance." This can be measured with various valid statistics:

 - Number of episodes
 - Number of cycles (steps)
 - Number of updates
 - Absolute computation time

Among these, we select the number of cycles (steps) needed to achieve certain performance as the primary measure, complemented with number of episodes. The number of cycles measure has the advantage of direct correlation with number of interactions with the environment.

### 4.3 Evaluating Controller Performance

Now let's discuss how to evaluate the performance of the agent. It is commonplace to average the "cost measure" over some number of episodes. For minimum-time controller problems, we use the average time to goal as the cost measure. For regulator problems, we measure the total time outside the target region.

Note that the learning controller can have a different internal goal formulation different from performance measure, either through reward shaping or discounting.

## 5 Empirical Results

### 5.1 The Pole Balancing Task

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/pole-balancing.gif" alt="Pole Balancing GIF"/>
  <figcaption>Pole Balancing</figcaption>
</figure>


In Pole Balancing, the agent's task is to balance a pole at an upright position. Its states are the pole angle $\theta$ and the angular velocity $\theta'$ by applying left, right, or zero force. The states are partitioned as follows:

| State | Condition                        | Cost |
|:-----:|:---------------------------------|------|
| $S^+$ | $\theta \in [-\pi/2, \pi/2]$     | 0    |
| $S^-$ | $\theta \not\in [-\pi/2, \pi/2]$ | 1    |

Three actions are available to the agent: 

 1. Apply left force ($-50 N$)
 2. Apply right force ($50 N$)
 3. Apply zero force ($0 N$)

A uniform noise of $[-10, 10]$ is added to the chosen action.

(For more details about the environment, please read Section 9.2 of *Least-Squares Policy Iteration* by Lagoudakis and Parr (2003))

For the NFQ, we use a multilayer-perceptron with two hidden layers. The input layer has 3 nodes: 2 for the state $\theta, \theta'$ and 1 for the action. The hidden layers have 5 nodes, and the output layer has 1 node. A sigmoid activation was used in all output.

To test our NFQ algorithm, we test it with various number of training episodes (50 to 400 episodes). The trained agent is then tested on 50 different initial states. NFQ succeeded in all 50 test episodes after 200 or more test episodes, which roughly translates to 1200 steps. 

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/table1.png" alt="Pole Balancing Results"/>
  <figcaption><strong>Table 1.</strong> Pole Balancing Results</figcaption>
</figure>

### 5.2 The Mountain Car Benchmark

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/mountain-car.gif" alt="Mountain Car GIF"/>
  <figcaption>Mountain Car</figcaption>
</figure>

In Mountain Car, the agent's task is to accelerate the car up to the top of the hill. Unfortunately, the agent's car is too weak to drive directly up the hill, so the agent must learn to move in the opposite direction to gain momentum. In this task, the states are the position $x$ and the velocity $x'$. 

| State | Condition         | Cost |
|:------|:------------------|------|
| $S^+$ | $x \geq 0.7$      | 0    |
| $S$   | $x \in (-1, 0.7)$ | 0.01 |
| $S^-$ | $x \leq -1$       | 1    |

Two actions are available to the agent: left (-4) and right (+4).

We use the same neural network architecture as the Pole Balancing problem. We use the hint-to-goal heuristic mentioned in Section 3.4, where we generate artificial data in the goal region. After every training episode, we generate 100 such artificial transitions.

To test our NFQ algorithm, we train an agent for up to 500 episodes, stopping early if the agent finds a successful policy. A successful policy is defined as a policy where the goal state is reached for 50 different test cases.

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/table2.png" alt="Mountain Car Results"/>
  <figcaption><strong>Table 2.</strong> Mountain Car Results</figcaption>
</figure>

### 5.3 The CartPole Regulator Benchmark

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/cartpole.gif" alt="CartPole GIF"/>
  <figcaption>CartPole</figcaption>
</figure>

In CartPole, the agent's task is to keep the pole in an upright position and the cart in the target position by moving the cart left and right. Because this is a regulator problem, there is no goal state. The goal of the agent is to be at the target position for as long as possible while staying away from the forbidden region.

| State | Condition                                            | Cost |
|:------|:-----------------------------------------------------|------|
| $S$   | $x \in [-0.05, 0.05]$ and $\theta \in [-0.05, 0.05]$ | 0    |
| $S$   | $x \in [-2.4, 2.4]$                                  | 0.01 |
| $S^-$ | $x \not\in [-2.4, 2.4]$                              | 1    |

Two actions are available to the agent: left (-10N) and right (10N).

We use the same neural network architecture as the Pole Balancing problem or the Mountain Car problem, except the input layer has 5 nodes instead of 3 since the state is represented by 4 numbers $s = (x, x', \theta, \theta')$.

We use both improvements mentioned in Section 3.4 Variants. We use the hint-to-goal heuristic, generating 100 artificial goal-region transition after each training episode. Also, when generating training episodes, we select actions greedily according to the current Q-value function instead of random actions.

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/table3.png" alt="CartPole Results"/>
  <figcaption><strong>Table 3.</strong> CartPole Results</figcaption>
</figure>

## 6 Conclusion

This paper proposes NFQ, a memory based method to train Q-value functions represented by a neural network. We show that storing and reusing experiences allow for greater data efficiency and more reliable learning and convergence. NFQ was able to learn successfully on 3 optimal control problems using an identical neural network structure, showing that NFQ is at least somewhat robust against hyperparameters.

<hr/>

## Final Thoughts

**Questions**
 - In the Pole Balancing problem, there are 3 possible actions: left, right, or no-op. However, in the Mountain Car and CartPole problem, there are only 2 actions: left or right. **What would happen if the no-op action was added**?
 - **How viable is the hint-to-goal heuristic?** It is very helpful in these three problems, but current deep reinforcement learning algorithms are tested on more complex environment suites such as *Atari 2600*. In such environments, the state representation is complex, and generating artificial data in the goal region can be difficult. Also, the state varies greatly within a state, so simply clamping the value in the goal region might not be as helpful.

**Recommended Next Papers**
 - Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013) [[Arxiv]](https://arxiv.org/abs/1312.5602)

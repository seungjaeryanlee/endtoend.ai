---
layout: post
title: "RL Weekly 26: Transfer RL with Credit Assignment and Convolutional Reservoir Computing for World Models"
author: Seungjae Ryan Lee
permalink: /rl-weekly/26
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/26/rcrc.png
image_type: contain
excerpt: "This week, we summarize a new transfer learning method using the Transformer reward model, and a world model controller that does not require training the feature extraction."

nav:
- name: "Transfer RL with Credit Assignment"
  permalink: "#transfer"
- name: "Convolutional Reservoir Computing for World Models"
  permalink: "#rcrc"
- name: "One-liners"
  permalink: "#one-liners"

related:
- title: "RL Weekly 25: Replacing Bias with Adaptive Methods, Batch Off-policy Learning, and Learning Shared Model for Multi-task RL"
  link: /rl-weekly/25
  image: /assets/blog/rl-weekly/25/bias.png
  image_type: contain
---


{% include revue.html %}


## Credit Assignment as a Proxy for Transfer in Reinforcement Learning {#transfer}

<p class="authors" style="font-size: 0.8em">
Johan Ferret<sup>1</sup>,
RaphaÃ«l Marinier<sup>1</sup>,
Matthieu Geist<sup>1</sup>,
Olivier Pietquin<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 0.8em">
    <sup>1</sup>Google Research, Brain Team
</p>

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/26/credit_and_transfer.png" alt="">
</div>

**What it says**

The "credit assignment" problem is a problem of identifying which actions influenced future rewards, and in what way these actions influenced those rewards. Giving valid credit to the important action is an important part of agent learning to solve the task, but it is difficult to pinpoint them. The authors propose using Transformer, a sequence-to-sequence (seq2seq) model with self-attention, to identify the relations between two state-action pairs. The Transformer is trained through a reward prediction task with partial states and actions given as input. The network must learn reconstruct the missing information from past state-action pairs to predict the reward accurately. (Section 3).

This Transformer network can be used to shape the reward. The shaped reward must follow the "reward shaping theorem" to induce the same optimal policies as the unshaped reward. The authors show that it is possible to formulate a state potential function using the self-attention weights (Section 4).

The authors point at the robustness to task modification as the main advantage of this reward model. Because this model is learned with credit assignment, the model is useful as long as the "causal structure underlying the reward function" remains unmodified. Thus, authors propose transfer learning by augmenting the reward signal of the new environment using this trained reward model (Section 5).

This algorithm is put to test in two environments: the 2D-grid Trigger environment and a 3D keys_doors_puzzle environment. In the Trigger environment, the agent must activate all the triggers to start receiving rewards, and in the Key-Door environment, the agent must get a colored key to unlock a same-colored door. The authors show that their algorithm can assign credit to key actions (activating the trigger or getting the key) very well (Section 6.1, Figure 3).

The algorithm is also tested for transfer learning, and verify that the agents train faster with the augmented rewards in both in-domain transfer (Figure 4) and out-of-domain transfer (Figure 5, 6). The plots show that reward shaping benefits agents in the very early phase of learning.

**Read more**

- [Credit Assignment as a Proxy for Transfer in Reinforcement Learning (ArXiv Preprint)](https://arxiv.org/abs/1907.08027)





## Convolutional Reservoir Computing for World Models {#rcrc}

<p class="authors" style="font-size: 0.8em">
Hanten Chang<sup>1</sup>,
Katsuya Futagami<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 0.8em">
    <sup>1</sup>Graduate school of Systems and Information Engineering, University of Tsukuba, Japan
</p>

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/26/rcrc.png" alt="">
</div>

**What it says**

Feature extraction is an important part of reinforcement learning, especially for environments with large observation space. World model is an algorithm that separates the training of feature extraction model (trained with VAE + MDN-RNN) and the action decision model (trained with CMA-ES), allowing the model to learn features not only to solve the task but to represent the whole environment well. (Section 2.2). The authors propose using "convolutional reservoir computing" to skip the training the feature extraction model and focusing on training the action decision model.

The proposed algorithm "RL with Convolutional Reservoir Computing" (RCRC) is inspired by reservoir computing (Section 2.1). The model uses a randomly initialized convolutional network with fixed weights to extract the features. The features are then given to "ESN", a reservoir computing model that use a fixed random matrix (Section 3.3). These outputs are given to the controller which is trained with CMA-ES, an evolution algorithm. (Section 3.4).

The authors test RCRC in the CarRacing environment and show that it is competitive to the classical world model controller, although it was slower to reach a stable high score (Section 4.3).


**Read more**

- [Convolutional Reservoir Computing for World Models (ArXiv Preprint)](https://arxiv.org/abs/1907.08040)

**External resources**

- [World Models (ArXiv Preprint)](https://arxiv.org/abs/1803.10122)
- [Recurrent World Models Facilitate Policy Evolution (ArXiv Preprint)](https://arxiv.org/abs/1809.01999)


------

<div id="one-liners"></div>

One-line introductions to some more exciting news in RL this week

- [**RLDM2019 Notes**](https://david-abel.github.io/notes/rldm_2019.pdf): David Abel published his notes for RLDM 2019!
- [**RAISIM**](https://www.reddit.com/r/reinforcementlearning/comments/ceubvl/raisim_physics_engine_for_robotics_and_ai_research/): Jemin Hwangbo relased RAISIM, a physics engine for robotics and AI research!
- [**MineRL Baselines**](https://github.com/minerllabs/quick_start/tree/master/chainerrl_baselines): ChainerRL released the baselines code for the NeurIPS 2019 MineRL competition!
- [**Walk Around**](https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around): The NeurIPS 2019 competition Learn to Move - Walk Around has begun!

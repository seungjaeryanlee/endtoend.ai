---
layout: post
title: "RL Weekly 12: Atari Demos with Human Gaze Labels, New SOTA in Meta-RL, and a Hierarchical Take on Intrinsic Rewards"
author: Seungjae Ryan Lee
permalink: /rl-weekly/12
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/12/16_games.png
image_type: contain
excerpt: "This week, we look at a new demo dataset of Atari games that include trajectories and human gaze. We also look at PEARL, a new meta-RL method that boasts sample efficiency and performance superior to previous state-of-the-art algorithms. Finally, we look at a novel method of incorporating intrinsic rewards."

nav:
- name: "Atari Human Eye-Tracking and Demo Dataset"
  permalink: "#atari-head-atari-human-eye-tracking-and-demonstration-dataset"
- name: "Efficient Off-Policy Meta-RL via Probabilistic Context Variables"
  permalink: "#efficient-off-policy-meta-rl-via-probabilistic-context-variables"
- name: "Hierarchical Incorporation of Intrinsic Rewards"
  permalink: "#hierarchical-incorporation-of-intrinsic-rewards"

related:
- title: "RL Weekly 11: The Bitter Lesson by Richard Sutton, the Promise of Hierarchical RL, and Exploration with Human Feedback"
  link: /rl-weekly/11
  image: /assets/blog/rl-weekly/11/combination.png
  image_type: contain
---


## Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/12/16_games.png" alt="Environments included in Atari-HEAD">
</div>

**What it is**

Atari-HEAD is a new dataset collected by researchers at UT Austin, Google, and CMU. The dataset contains human actions and eye-movements playing Atari 2600 games on the Arcade Learning Environment (ALE). The dataset contains demonstration data for 16 games (Asterix, Berzerk, Breakout, Centipede, Demon Attack, Enduro, Freeway, Frostbite, Hero, Ms. Pacman, Name This Game, Phoenix, Riverraid, Seaquest, Space Invaders, and Venture). To obtain near-optimal decision, the human players played these games frame-by-frame.

**Why it matters**

Imitation Learning is a powerful technique that allows agents to learn with little amount of interaction. However, imitation learning requires demonstration data, and gathering such data is difficult and expensive. As a result, only labs with access to such data were able to study imitation learning on various environments. This open dataset allows for labs to test their imitation learning algorithms on the most standard RL testbed: Arcade Learning Environment (ALE). Furthermore, the human gaze position data with the trajectories allows for imitation learning with visual attention.

**Read more**

- [Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset (ArXiv Preprint)](https://arxiv.org/abs/1903.06754)
- [Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset (Dataset)](https://zenodo.org/record/2603190)

**External Resources**

- [The Atari Grand Challenge Dataset](https://arxiv.org/abs/1705.10998)
- [Atari Grand Challenge v2 data (Dataset)](http://atarigrandchallenge.com/)



## Efficient Off-Policy Meta-RL via Probabilistic Context Variables

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/12/pearl_perf.png" alt="Performance of PEARL">
</div>

**What it is**

Researchers at UC Berkeley devloped Probabilistic Embeddings for Actor-critic RL (PEARL), an off-policy meta-RL algorithm. Meta-RL refers to methods that can adapt and learn quickly in new tasks by incorporating experience from related tasks. The authors show that PEARL is drastically more sample efficient while outperforming other meta-RL methods (RL^2, MAML, and ProMP).

**Read more**

- [Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables (ArXiv Preprint)](https://arxiv.org/abs/1903.08254)
- [PEARL: Efficient Off-policy Meta-learning via Probabilistic Context Variables (GitHub Repo)](https://github.com/katerakelly/oyster)

**External Resources**

These are resources to learn about meta learning:

- [Meta-Learning: Learning to Learn Fast (Lilian Weng's Blog Post)](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)
- [Meta Learning (CS294-112: Deep Reinforcement Learning Lecture Slides)](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-20.pdf)



## Hierarchical Incorporation of Intrinsic Rewards

<div class="w60" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/12/flytrapescape.png" alt="Successor Features in FlytrapEscape">
</div>

**What it is**

Researchers at University of Freiburg proposed Scheduled Intrinsic Drive (SID) agent that separates maximization of intrinsic and extrinsic rewards. Instead of maximizing the sum of intrinsic and extrinsic rewards, a high-level controller chooses which reward to maximize for certain number of steps. The authors claim that random controller is a good candidate. The authors also propose Successor Feature Control (SFC), a new intrinsic reward using successor features for more far-sighted exploration. SID and SFC are shown to perform better than simple sum and ICM or RND in navigation tasks.

**Why it matters**

Existing state-of-the-art methods such as Intrinsic Curiosity Module (ICM) and Random Network Distillation (RND) provide novel methods of representing novel experiences as numerical intrinsic rewards. However, they do not provide novel methods of combining intrinsic rewards and extrinsic rewards and simply add them and provide it to the agent as a reward signal. Because these rewards stem from different objectives, different combining methods such as SID would be an interesting avenue to pursue to improve agent's performance.

**Read more**

- [Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration (ArXiv Preprint)](https://arxiv.org/abs/1903.07400)
- [Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration (YouTube Video)](https://www.youtube.com/watch?v=4ZHcBo7006Y)


---

Some more exciting news in RL:

- Marc G. Bellemare published a blog post summarizing [his 18 months of RL research at Google Brain in Montreal](http://www.marcgbellemare.info/blog/eighteen-months-of-rl-research-at-google-brain-in-montreal/).
- Joshua Achiam, Ethan Knight, and Pieter Abbeel [investigated causes of divergence in Deep Q-Learning and proposed Preconditioned Q-Networks (PreQN)](https://arxiv.org/abs/1903.08894).
- [ICML Workshops](https://icml.cc/Conferences/2019/Schedule) have been announced. Workshops such as "Exploration in Reinforcement Learning Workshop", "Workshop on Multi-Task and Lifelong Reinforcement Learning", or ["Reinforcement Learning for Real Life"](https://sites.google.com/view/RL4RealLife) could be a nice place to submit your research!

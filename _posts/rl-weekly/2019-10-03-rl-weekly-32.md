---
layout: post
title: "RL Weekly 32: New SotA Sample Efficiency on Atari and an Analysis of the Benefits of Hierarchical RL"
author: Seungjae Ryan Lee
permalink: /rl-weekly/32
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/32/laser_sample_efficiency.png
image_type: contain
excerpt: "In this issue, we look at LASER, DeepMind's improvement to V-trace that achieves state-of-the-art sample efficiency in Atari environments. We also look at Google AI and UC Berkeley's study on hierarchical RL, analyzing and isolating the reason behind the benefits of hierarchical RL."

nav:
- name: "Off-Policy Actor-Critic with Shared Experience Replay"
  permalink: "#laser"
- name: "Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?"
  permalink: "#why-hierarchy"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 31: How Agents Play Hide and Seek, Attraction-Repulsion Actor Critic, and Efficient Learning from Demonstrations"
  link: /rl-weekly/31
  image: /assets/blog/rl-weekly/31/hide-and-seek-front.png
  image_type: cover
---


{% include revue.html %}

<style>
.letter, .letter p {
  color: gray;
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
.letter a {
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
</style>

<div class="letter">
<p>
Dear readers,
</p>
<p>
Here is the 32nd issue! In this week's issue, there are two new algorithms that boast state-of-the-art in the Atari environment suite. There are also fascinating new frameworks, especially for robotics.
</p>
<p>
Meanwhile, I am going to Microsoft Research's Reinforcement Learning Day at New York today. It would be great to meet anyone who is also visiting!
</p>
<p>
As always, please feel free to <a href="mailto:seungjaeryanlee@gmail.com">email me</a> or <a href="https://forms.gle/yZiHUXbtph8msVHn9">leave any feedback</a>.
</p>
<p>
- Ryan
</p>
</div>


## Off-Policy Actor-Critic with Shared Experience Replay {#laser}

<p class="authors" style="font-size: 1em">
Simon Schmitt<sup>1</sup>,
Matteo Hessel<sup>1</sup>,
Karen Simonyan<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>DeepMind
</p>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/32/laser_sample_efficiency.png" alt="">
</div>


**What it says**

V-trace is an importance sampling technique introduced in the IMPALA paper (Espeholt et al., 2018) for off-policy correction. It uses clipped importance sampling ratios which reduces the variance of the estimated return but introduces bias. The authors prove that this is problematic by showing that even when given an optimal value function, V-trace may not converge to a locally optimal policy. The authors suggest mixing on-policy data into the V-trace policy gradient and show that sufficient amount of online data alleviates the problem. (Section 3).

The authors also propose using trust region estimators. Off-policy learning algorithms often use multiple behavior policies to collect experience, and some of them may be unsuitable. Using importance sampling for those behavior policies may result in high or even infinite variance. Thus, the authors define a behavior relevance function and use a trust region scheme to adaptively select only suitable behavior distributions to estimate the value. (Section 4).

The authors test their algorithm LASER (LArge Scale Experience Replay) on Atari and DMLab-30 and show that it is more sample-efficient than Rainbow, Reactor, and IMPALA Meta-gradient. Interestingly, they show that prioritized experience replay did not meaningfully improve LASER's performance (Section 5.2).

**Read more**

- [arXiv Preprint](https://arxiv.org/abs/1909.11583)

**External resources**

- [IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (arXiv Preprint)](https://arxiv.org/abs/1802.01561): Introduced V-trace (Section 4)
- [A Deeper Look at Experience Replay (arXiv Preprint)](https://arxiv.org/abs/1712.01275): Introduced Combined Experience Replay (DQN) with similar idea.





## Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning? {#why-hierarchy}

<p class="authors" style="font-size: 1em">
Ofir Nachum<sup>1</sup>,
Haoran Tang<sup>2</sup>,
Xingyu Lu<sup>2</sup>,
Shixiang Gu<sup>1</sup>,
Honglak Lee<sup>1</sup>,
Sergey Levine<sup>12</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Google AI,
    <sup>1</sup>UC Berkeley
</p>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/32/why_hierarchy_summary.png" alt="">
</div>

**What it says**

Hierarchical RL (HRL) have been successful in various environments, achieving performance comparable to or better than "shallow" RL that does not use hierarchy. Despite such success, the benefits of HRL is not understood well, and the justification to HRL largely consists of arguments based on intuition. 

To analyze the benefits of HRL, the authors define 4 hypotheses for the benefits of HRL (shown above) and verify them on 4 simulation tasks of a quadrupedal robot. The results show that although temporally abstract training helps performance, it comes from multi-step rewards, and that the main benefit of HRL comes from exploration.

The authors also test how important it is to have separate high-level and low-level policies (modularity). Experiments show that agents with high-level and low-level policies using the same network with multiple heads performed consistently worse than those with separate networks, showing that modularity is important (Appendix B).

**Read more**

- [arXiv Preprint](https://arxiv.org/abs/1909.10618)





------

<div id="more-news"></div>

Here are some more exciting news in RL:

[**V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control**](https://arxiv.org/abs/1909.12238)
<br/>
An on-policy variant of Maximum a Posteriori Policy Optimization (MPO) that achieves state-of-the-art score in the multi-task setting of Atari-57 and DMLab-30.

[**Multiagent Evaluation under Incomplete Information**](https://arxiv.org/abs/1909.09849)
<br/>
Introduces a multiagent evaluation method that does not assume noise-free game outcomes and with its theoretical guarantees and evaluation results.

[**Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?**](https://arxiv.org/abs/1909.10618)
<br/>
Experimentally determines that hierarchical RL is better than standard RL for learning over temporally extended periods due to improved exploration. Presents two non-hierarchical exploration techniques motivated from hierarchical RL that achieve competitive performance.

[**RLBench: The Robot Learning Benchmark & Learning Environment**](https://sites.google.com/view/rlbench)
<br/>
A collection of 100 robot learning tasks built around PyRep with expert algorithms for demonstration generation.

[**ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots**](https://sites.google.com/view/roboticsbenchmarks/)
<br/>
Open-source platform for affordable RL robots for three-fingered hand robot (dexterous manipulation) and four-legged robot (locomotion).

[**Avoidance Learning Using Observational Reinforcement Learning**](https://arxiv.org/abs/1909.11228)
<br/>
Learn to avoid a bad demonstrator policy to avoid dangerous behaviors through a pseudocount avoidance bonus.

[**Revisit Policy Optimization in Matrix Form**](https://arxiv.org/abs/1909.09186)

[**A Framework for Data-Driven Robotics**](https://arxiv.org/abs/1909.12200)
<br/>
Using demonstrations of human teleoperating the robot with some reward annotations, learn a reward model and use it to train an agent with off-policy batch RL.

[**Leveraging Human Guidance for Deep Reinforcement Learning Tasks**](https://arxiv.org/abs/1909.09906)
<br/>
A survey on different methods of using human guidance for reinforcement learning.

[**Harnessing Structures for Value-Based Planning and Reinforcement Learning**](https://arxiv.org/abs/1909.12255)
Shows that Q-values of many Atari environments have low-rank structures and that recovering and using this structure allows for higher scores with Dueling DQN.




<!-- 
APPLICATION
- [RL for Attraction Selection](https://arxiv.org/abs/1909.10500)
- [RL for Portfolio Management](https://arxiv.org/abs/1909.09571)
- [RL for Intelligent Traffic Control](https://arxiv.org/abs/1909.10651)
- [RL for Active Perception Image Classification](https://arxiv.org/abs/1909.09705)
- [RL for Civilian UAV Management](https://arxiv.org/abs/1909.10090)
- [RL for Redirection](https://arxiv.org/abs/1909.09505)
- [RL for Driver Drowsiness](https://arxiv.org/abs/1909.11456)
- [RL for Lane Changes](https://arxiv.org/abs/1909.11538)
- [RL for Source Seeking](https://arxiv.org/abs/1909.11236)
- [Motion Retargetting](https://arxiv.org/abs/1909.11303)
- [UAV Power Management](https://arxiv.org/abs/1909.12217)
- [SAT Solver](https://arxiv.org/abs/1909.11830)
- [RL for Data Valuation](https://arxiv.org/abs/1909.11671) -->

<!-- - https://arxiv.org/abs/1909.11730 (SPOT -> EVT) -->
<!-- - https://arxiv.org/abs/1909.11373 (tiMe) -->
<!-- - https://arxiv.org/abs/1909.11821 (MBRL+WGAN) -->
<!-- - https://arxiv.org/abs/1909.10449 (Sim2Real, ROMDP) -->
<!-- - [**Deep Dynamics Models for Learning Dexterous Manipulation**](https://sites.google.com/view/pddm/) -->
<!-- [**How Much Do Unstated Problem Constraints Limit Deep Robotic Reinforcement Learning?**](https://arxiv.org/abs/1909.09282) -->
<!-- - [Invariant Transform Experience Replay](https://arxiv.org/abs/1909.10707) -->
<!-- [**Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Estimators for Reinforcement Learning**](https://arxiv.org/abs/1909.10549) -->

---
layout: post
title: "RL Weekly 5: Robust Control of Legged Robots, Compiler Phase-Ordering, and Go Explore on Sonic the Hedgehog"
author: Seungjae Ryan Lee
permalink: /rl-weekly/5
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/5/legged_robot_summary.jpg
front_image_type: contain
excerpt: >
   This week, we look at impressive robust control of legged robots by ETH Zurich and Intel, compiler phase-ordering by UC Berkeley and MIT, and a partial implementation of Uber's Go Explore.

nav:
- name: Robust Control of Legged Robots
  permalink: '#learning-agile-and-dynamic-motor-skills-for-legged-robots'
- name: Compiler Phase-Ordering
  permalink: '#autophase-compiler-phase-ordering-with-deep-rl'
- name: Go Explore on Sonic
  permalink: '#go-explore-on-sonic-the-hedgehog'
---


## Learning Agile and Dynamic Motor Skills for Legged Robots

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/5/legged_robot_summary.jpg" alt="Summary of approach">
</div>

**What it is**

Researchers at ETH Zurich and Intel showed that deep reinforcement learning methods are capable of training robust policy for a four-legged robot on a simulated environment. To handle simulation inaccuracies from actuator dynamics, joint damping, or signal delays, they propose learning an "actuator network" that maps commanded actions to resulting torques in an end-to-end manner. Thanks to their efficient software implementation, the agent can learn a robust control policy in just 4-11 hours on a normal desktop machine with 1 CPU and GPU. The agent can recover from various fallen positions and maintain balance when kicked.

**Why it matters**

Previous results of robotic control using reinforcement learning have been disappointing: policies trained with reinforcement learning were neither robust nor data efficient. Thus, some companies have focused on classic control methods instead of reinforcement learning. This result is very impressive in both its efficiency and its robustness. 4-11 hours on a normal desktop machine is a minute amount of computation compared to most reinforcement learning methods (For reference, training a DQN on a single Atari 2600 games would take longer). Furthermore, the trained agent is extremely robust, maintaining balance after being kicked numerous times. We invite the readers to watch the YouTube video in the link below.


**Read more**

- [Learning Agile and Dynamic Motor Skills for Legged Robots (Science Robotics)](http://robotics.sciencemag.org/content/4/26/eaau5872)
- [Learning Agile and Dynamic Motor Skills for Legged Robots (PowerPoint slides)](http://robotics.sciencemag.org/content/4/26/eaau5872.powerpoint)
- [Learning Agile and Dynamic Motor Skills for Legged Robots (YouTube Video)](https://www.youtube.com/watch?v=aTDkYFZFWug)

**External Resources**

- [[R] Real robot trained via simulation and reinforcement learning is capable of running, getting up and recovering from kicks (Reddit /r/MachineLearning)](https://www.reddit.com/r/MachineLearning/comments/ahm5u3/r_real_robot_trained_via_simulation_and/)


## AutoPhase: Compiler Phase-Ordering with Deep RL

<div class="w60" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/5/autophase_diagram.png" alt="Block Diagram of AutoPhase">
</div>

**What it is**

There are numerous methods for optimizing the compiler. Unfortunately, it is known that the order of optimizations performed significantly impacts the performance. This "phase-ordering problem" is an NP-hard problem, so most existing "solutions" use heuristic search algorithms, with few using machine learning or neuroevolution methods. Instead, researchers at UC Berkeley and MIT propose using deep reinforcement learning algorithm to find a sequence of passes to optimize the performance of these programs. The authors report 16% improvement over -O3 compiler flag while being one or two orders of magnitude faster.

**Why it matters**

Most previous approaches of compiler phase-ordering problem relied on handcrafted heuristic search. The authors show that using DQN or PG outperforms the traditional -O3 compiler flag and has similar results as genetic algorithms. Because they used "vanilla" reinforcement learning algorithms, we can expect better results with more complex reinforcement learning algorithms. Also, it would be interesting to if similar enhancements would occur for related compiler optimization tasks. Overall, it is exciting to see another possible use case of deep reinforcement learning.

**Read more**

- [AutoPhase: Compiler Phase-Ordering for High-Level Synthesis with Deep Reinforcement Learning (arXiv paper)](https://arxiv.org/abs/1901.04615)



## Go Explore on Sonic the Hedgehog

![Playthough of a Green Hill Zone Act 1](https://i.imgur.com/SjArtkz.gif)

**What it is**

Go Explore was presented by Uber last November that showed state of the art performance in hard exploration games given domain knowledge (72543 on Montezuma's Revenge, 4655 on Pitfall). Unfortunately, Uber has yet to release either a paper or a source code of Go Explore, perhaps due to the amount of community feedback after the blog post. "R-McHenry" (GitHub username) partially implemented this algorithm and test it on Sonic the Hedgehog. 

**Why it matters**

Despite its state of the art performance, Go Explore is still a mystery in large. The author of the code says that "the code is currently a pretty dirty/embarrassing python notebook", but it is certainly a worthwhile step towards verifying the reproducibility of Go Explore.

**Read more**

- [[D] Go Explore VS Sonic the Hedgehog (Reddit /r/MachineLearning)](https://www.reddit.com/r/MachineLearning/comments/agf43s/d_go_explore_vs_sonic_the_hedgehog/)
- [[P] Parallelized Go Explore (Reddit /r/MachineLearning)](https://www.reddit.com/r/MachineLearning/comments/ahh3ss/p_parallelized_go_explore/)
- [SynchronousGoExplore (GitHub Repository)](https://github.com/R-McHenry/SynchronousGoExplore)

**External Resources**

- [Montezuma’s Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems (Uber Blog Post)](https://eng.uber.com/go-explore/)
- [[R] Montezuma’s Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems (Reddit /r/MachineLearning)](https://www.reddit.com/r/MachineLearning/comments/a0nnp7/r_montezumas_revenge_solved_by_goexplore_a_new/)

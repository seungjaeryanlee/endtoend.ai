---
layout: post
title: "RL Weekly 40: Catastrophic Interference and Policy Evaluation Networks"
author: Seungjae Ryan Lee
permalink: /rl-weekly/40
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/40/montezuma_breakout.png
image_type: contain
excerpt: "In this issue, we look at two papers combating catastrophic interference. Memento combats interference by training two independent agents where the second agent takes off when the first agent is finished. D-NN and TC-NN reduce interference by mapping the input space to a higher-dimensional space. We also look at Policy Evaluation Network, a network that predicts the expected return given a policy."

nav:
- name: "On Catastrophic Interference in Atari 2600 Games"
  permalink: "#memento"
- name: "Improving Performance in Reinforcement Learning by Breaking Generalization in Neural Networks"
  permalink: "#breaking-generalization"
- name: "Policy Evaluation Networks"
  permalink: "#policy-evaluation-networks"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 39: Intrinsic Motivation for Cooperation and Amortized Q-Learning"
  link: /rl-weekly/39
  image: /assets/blog/rl-weekly/39/synergistic_overview.png
  image_type: contain
---


{% include revue.html %}

<style>
.letter, .letter p {
  color: gray;
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
.letter a {
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
</style>

<div class="letter">
<p>
Dear readers,
</p>
<p>
With COVID-19, we are going through difficult times, both as individuals and as members of society. Personally, I had to fly back to Korea and self-isolate. I sincerely wish that you, your family and your friends all stay safe.
</p>
<p>
On a more positive note, I am glad to say that RL Weekly reached 1000 subscribers. When I was writing my first issue, I did not imagine that 1000 people would read what I wrote. I will continue to collect exciting news in RL and summarize them to the best of my abilities. I hope you will stay with me on this wonderful journey!
</p>
<p>
In this issue, we look at two papers combating catastrophic interference. Memento combats interference by training two independent agents where the second agent takes off when the first agent is finished. D-NN and TC-NN reduce interference by mapping the input space to a higher-dimensional space. We also look at Policy Evaluation Network, a network that predicts the expected return given a policy.
</p>
<p>
I would love to hear your feedback! If you have anything to say, please <a href="mailto:seungjaeryanlee@gmail.com">email me</a>.
</p>
<p>
- Ryan
</p>
</div>



## On Catastrophic Interference in Atari 2600 Games {#memento}

<p class="authors" style="font-size: 1em">
William Fedus<sup>*12</sup>,
Dibya Ghosh<sup>*1</sup>,
John D. Martin<sup>13</sup>,
Marc G. Bellemare<sup>14</sup>,
Yoshua Bengio<sup>25</sup>,
Hugo Larochelle<sup>14</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Google Brain
    <sup>2</sup>MILA, Université de Montréal
    <sup>3</sup>Stevens Institute of Technology 
    <sup>4</sup>CIFAR Fellow
    <sup>5</sup>CIFAR Director
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/pong_qbert.png" alt="">
</div>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/montezuma_breakout.png" alt="">
</div>

**What it says**

Catastrophic forgetting (or catastrophic interference) is a term that describes the phenomenon where learning from one experience results in the agent "forgetting" what it has learned previously with different experiences. It has commonly been discussed in multitask learning where the agent must learn different tasks, but the authors show that this can also happen in a single environment.

The authors establish game "context" in Arcade Learning Environment (ALE) using the game scores. Then, the authors visualize catastrophic interference by measuring the TD-errors across all contexts after training for a fixed number of steps with one of the contexts in the replay buffer. The top figures show that for Pong and Qbert, learning generalizes well between contexts, as training in one context reduces the TD-error for most of the other contexts. (Blue color denotes reduced TD-error, and red color denotes increased TD-error) However, the bottom figures show that for Montezuma's Revenge and Breakout, learning does not generalize well between contexts. In fact, the only blue regions are the diagonal lines, so the agent learning in one context only learns about that context and "forgets" all the other contexts.

To combat this interference, the authors propose Memento, an idea of initializing a second independent agent that "begins each of its episode from the final position of the last." Since the two agents are independent, there cannot be any interference. Using DQN and Rainbow, the authors show that Memento performs well on hard exploration Atari games such as Gravitar, Venture, and Private Eye. Furthermore, they also show that simply training one agent for twice the duration or with twice the model capacity does not perform as well as Memento, allowing them to attribute Memento's enhanced performance to reducing interference.

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/2002.12499)
- [Twitter Thread (Dibya Ghosh)](https://twitter.com/its_dibya/status/1234516717268762624)
- [GitHub](https://github.com/google-research/google-research/tree/master/memento)
- [Talk at NeurIPS 2019 BARL Workshop](https://slideslive.com/38924023/memento-further-progress-through-forgetting)





## Improving Performance in Reinforcement Learning by Breaking Generalization in Neural Networks {#breaking-generalization}

<p class="authors" style="font-size: 1em">
Sina Ghiassian<sup>1</sup>,
Banafsheh Rafiee<sup>1</sup>,
Yat Long Lo<sup>1</sup>,
Adam White<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta and Alberta Machine Intelligence Institute (AMII)
</p>

<div class="w50">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/tile_coding.png" alt="">
</div>

**What it says**

Catastrophic interference is a phenomenon where "later training decreases performance by overriding previous learning." The authors suggest that this phenomenon could be one of the causes of deep RL algorithms being sensitive to hyperparameters and architecture choices. The authors claim that although generalization is a powerful ability, there can be "inappropriate generalization" of the inputs. For example, for Mountain Car environment where the observations are the position and the velocity of the car, " the value function exhibits significant discontinuities." Therefore, the authors suggest reducing this harmful generalization by mapping the observations to a higher-dimensional space. The authors test two methods: (1) "binning to discretize each dimension of the input separately," (D-NN) and (2) tile coding as shown in the figure above (TC-NN).

The authors test their algorithm on 1 prediction problem on Mountain Car and 2 control problems on Mountain Car and Acrobot. The results show that such methods improve performance (Section 5.1, Figure 3), are generally less sensitive to step-size (Section 5.2, Figure 4), and have reduced interference (Section 5.3, Figure 5).

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/2003.07417)





## Policy Evaluation Networks {#policy-evaluation-networks}

<p class="authors" style="font-size: 1em">
Jean Harb<sup>12</sup>,
Tom Schaul<sup>2</sup>,
Doina Precup<sup>2</sup>,
Pierre-Luc Bacon<sup>3</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Mila - McGill University
    <sup>2</sup>DeepMind
    <sup>3</sup>Mila - University of Montreal
</p>

<div class="w50">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/pvn_diagram.png" alt="">
</div>

<div class="w50">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/pvn_algorithm.png" alt="">
</div>

**What it says**

In most RL algorithms, value functions play a central role in finding a good policy, and thus it is trained with a lot of transitions. The transitions that are used to train this value function are dependent on the current policy, so the value function estimate is "influenced by previously seen policies, possibly in an unpredictable way." To "generalize the value function among different policies" authors suggest giving some description of the policy as an input. The authors propose Policy Evaluation Network (PVN), a neural network that predicts the expected cumulative reward of a given policy. PVN is fully differentiable and can be trained with supervised learning. The PVN can be trained with a dataset of pairs of randomly initialized policies and corresponding expected returns obtained through Monte-Carlo rollouts. After training, the policy can be improved with gradient ascent in the space of parametrized policies without environment interaction.

To represent a policy to give as the input to the PVN, simply flattening and concatenating layers of the policy network may lose valuable information on the dependencies between layers. Therefore, the authors propose "network fingerprinting," where the policy is characterized by the outputs when it is given a set of "probing states." These probing states synthetic inputs with the purpose of accurately representing the policy. To choose the probing states, the authors randomly initialize them and train them using backpropagation along PVN. (This is possible since the entire PVN is differentiable.)

The authors test PVN on 2-state MDP, CartPole, and MuJoCo Swimmer. The results in CartPole show that network fingerprinting is necessary for multilayered policy networks, and the results in Swimmer show that PVN can perform better than DDPG, TD3, and SAC.

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/2002.11833)
- [Twitter Thread (Jean Harb)](https://twitter.com/JeanHarb/status/1235666424300859393)





------

<div id="more-news"></div>

Here is some more exciting news in RL:

<!-- [**TODO**](todo)
<br/>
TODO -->

[**Maxmin Q-learning: Controlling the Estimation Bias of Q-learning**](https://arxiv.org/abs/2002.06487)
<br/>
Reduce the overestimation bias by maintaining multiple estimates of the action value and using the minimum of them as the Q-learning target.

[**Estimating Q(s,s') with Deep Deterministic Dynamics Gradients**](https://sites.google.com/view/qss-paper)
<br/>
Create a value function that estimates the return when transitioning from one state to another. This "decouples actions from values" and allows (1) off-policy learning with observations from suboptimal or random policies and (2) learning on environments with redundant action spaces.

[**TWIML: Advancements in Reinforcement Learning with Sergey Levine**](https://twimlai.com/twiml-talk-355-advancements-in-reinforcement-learning-with-sergey-levine/)
<br/>
This is a 42-minute TWIML AI Podcast with Sergey Levine.

[**TWIML: Upside-Down Reinforcement Learning with Jürgen Schmidhuber**](https://twimlai.com/twiml-talk-357-upside-down-reinforcement-learning-with-jurgen-schmidhuber/)
<br/>
This is a 33-minute TWIML AI Podcast with Jürgen Schmidhuber.

[**Self-Tuning Deep Reinforcement Learning**](https://arxiv.org/abs/2002.12928)
<br/>
Use metagradients to self-tune important hyperparameters of IMPALA such as discount factor, bootstrapping parameter, and loss weights, resulting in better performance.

[**Jelly Bean World: A Testbed for Never-Ending Learning**](https://arxiv.org/abs/2002.06306)
<br/>
A new 2D gridworld environment for lifelong learning with vision and "scent" observations.


<!-- [**Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft**](https://arxiv.org/abs/2003.06066)
<br/>
This is the whitepaper for the solution that placed 3rd in the NeurIPS 2019 MineRL competition. -->

<!-- - [Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration](https://arxiv.org/abs/2002.09253) -->
<!-- - [How Transferable are the Representations Learned by Deep Q Agents?](https://arxiv.org/abs/2002.10021) -->
<!-- - [RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments](https://arxiv.org/abs/2002.12292) -->
<!-- - [How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents](https://arxiv.org/abs/2002.08795) -->
<!-- - [REST: Performance Improvement of a Black Box Model via RL-based Spatial Transformation](https://arxiv.org/abs/2002.06610) -->
<!-- - [Review, Analyze, and Design a Comprehensive Deep Reinforcement Learning Framework](https://arxiv.org/abs/2002.11883) -->
<!-- - [TanksWorld: A Multi-Agent Environment for AI Safety Research](https://arxiv.org/abs/2002.11174) -->
<!-- - [Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement](https://arxiv.org/abs/2002.11089) -->
<!-- - [Discriminative Particle Filter Reinforcement Learning for Complex Partial Observations](https://arxiv.org/abs/2002.09884) -->
<!-- - [ConQUR: Mitigating Delusional Bias in Deep Q-learning](https://arxiv.org/abs/2002.12399) -->
<!-- - [2003.06259] Taylor Expansion Policy Optimization -->
<!-- - [2003.05861] The Chef's Hat Simulation Environment for Reinforcement-Learning-Based Agents -->
<!-- - [2003.05878] Option Discovery in the Absence of Rewards with Manifold Analysis -->
<!-- - [2003.06082] Action for Better Prediction -->
<!-- - [2003.06016] Invariant Causal Prediction for Block MDPs -->
<!-- - [2003.05988] Analysis of Hyper-Parameters for Small Games_ Iterations or Epochs in Self-Play_ -->

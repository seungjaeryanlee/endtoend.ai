---
layout: post
title: "RL Weekly 41: Adversarial Policies, Image Augmentation, and Self-Supervised Exploration with World Models"
author: Seungjae Ryan Lee
permalink: /rl-weekly/41
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/41/adversarial-policies-cover.png
image_type: contain
excerpt: "In this issue, we look at adversarial policy learning, image augmentation in RL, and self-supervised exploration through world models."

nav:
- name: "Adversarial Policies"
  permalink: "#Adversarial-Policies"
- name: "DrQ"
  permalink: "#DrQ"
- name: "Plan2Explore"
  permalink: "#Plan2Explore"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 40: Catastrophic Interference and Policy Evaluation Networks"
  link: /rl-weekly/40
  image: /assets/blog/rl-weekly/40/montezuma_breakout.png
  image_type: contain
---


{% include revue.html %}

<style>
.letter, .letter p {
  color: gray;
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
.letter a {
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
</style>

<div class="letter">
<p>
Dear readers,
</p>
<p>
In this issue, we look at adversarial policy learning, image augmentation in RL, and self-supervised exploration through world models.
</p>
<p>
I would love to hear your feedback! If you have anything to say, please <a href="mailto:seungjaeryanlee@gmail.com">email me</a>.
</p>
<p>
- Ryan
</p>
</div>

## Adversarial Policies: Attacking Deep Reinforcement Learning {#Adversarial-Policies}

<p class="authors" style="font-size: 1em">
Adam Gleave<sup>1</sup>,
Michael Dennis<sup>1</sup>,
Cody Wild<sup>1</sup>,
Neel Kant<sup>1</sup>,
Sergey Levine<sup>1</sup>,
Stuart Russell<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>UC Berkeley
</p>

<div class="w70">
  <video autoplay muted loop controls style="max-width: 100%;" src="https://adversarial-policies-public.s3.amazonaws.com/videos/YouShallNotPassHumans-v0_victim_ZooV1_opponent_Adv1_720p.mp4"></video>
</div>

<div class="w70">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/41/adversarial-policies-envs.png" alt="4 environments used to test adversarial policies">
</div>

**What it says**

In one-player RL benchmarks Atari or MuJoCo, the environment does not actively try to make agents fail. In contrast, in zero-sum games such as Chess or Go, the opponent strives to crush the agent. Such a setting requires the agent to be robust against diverse opponent strategies, and many researchers have used self-play to ensure robustness. However, what happens if an adversarial opponent is directly trained against the agent?

The authors show that the adversarially trained opponents could reliably beat the "victims", not by becoming better at the task, but by creating adversarial observations. Four two-player robotics games shown above are used as environments, and the adversaries are trained with Proximal Policy Optimization (PPO) with a sparse reward for match results. The video above shows the  "You Shall Not Pass" environment, where the runner's goal is to reach the finish line, and the blocker's goal is to prevent that. In the video, the trained adversary (red humanoid) never stands up, yet the victim (blue humanoid) struggles to reach the finish line.

The authors offer further analysis of this phenomenon. Against humanoid with random actions and one with no actions, the victim consistently wins, showing that the victim policies are "robust to off-distribution observations that are not adversarially optimized." The authors also test "masked" victims that cannot observe the adversary's position and find that the masked victim can reliably win against the adversary, although it struggles against normally trained opponents. Finally, the authors show that victims that are fine-tuned against an adversary is robust against that particular adversary, but is still vulnerable to a different adversary trained with this same method against the fine-tuned victim.

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/1905.10615)
- [Blog Post](https://bair.berkeley.edu/blog/2020/03/27/attacks/)
- [GitHub Page](https://adversarialpolicies.github.io/)
- [Twitter Thread](https://twitter.com/ARGleave/status/1243373829650395138)



## Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels {#DrQ}

<p class="authors" style="font-size: 1em">
Ilya Kostrikov<sup>1*</sup>,
Denis Yarats<sup>12*</sup>,
Rob Fergus<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>New York University
    <sup>2</sup>Facebook AI Research
</p>

<div class="w70">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/41/drq_algorithm.png" alt="">
</div>


**What it says**

Data Augmentation is widely used in vision and speech domains, but it is not often used in RL, especially in non-robotics domains. (In robotics, OpenAI used extensive data augmentation for [dexterous in-hand manipulation](https://openai.com/blog/solving-rubiks-cube/), and Hindsight Experience Replay can also be seen as a type of data augmentation.) To justify the need for data augmentation, the authors train Soft Actor-Critic (SAC) on the DeepMind control suite using image encoder architectures of various sizes while keeping the policy network architecture the same. The performance decreases as the parameter count of the encoder network increases, indicating that there is an overfitting problem. The authors propose Data-regularized Q (DrQ), an algorithm that uses image augmentation in RL to perturb input observations and regularize the Q-function.

DrQ can be divided into three parts, denoted Orange, Green, and Blue in the pseudocode above. The first augmentation is performed on the observations sampled from the replay buffer by shifting the 84-by-84 image by 4 pixels. The second augmentation is done on target Q-function by estimating Q-values of augmented observations and taking the mean of those values to reduce the variance. Similarly, the third augmentation is done on the Q-function.

In DeepMind control suite, DrQ with SAC achieves equal or better performance compared to CURL, PlaNet, SAC-AE, and SLAC, with scores comparable to SAC trained with internal states.

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/2004.13649)
- [GitHub Repo](https://github.com/denisyarats/drq)
- [Google Site](https://sites.google.com/view/data-regularized-q)
<!-- - [Twitter](https://twitter.com/ak92501/status/1255318560836136961) -->



## Planning to Explore via Self-Supervised World Models {#Plan2Explore}

<p class="authors" style="font-size: 1em">
Ramanan Sekar<sup>1*</sup>,
Oleh Rybkin<sup>1*</sup>,
Kostas Daniilidis<sup>1</sup>,
Pieter Abbeel<sup>2</sup>,
Danijar Hafner<sup>34</sup>,
Deepak Pathak<sup>56</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>University of Pennsylvania
    <sup>2</sup>UC Berkeley
    <sup>3</sup>Google Research, Brain Team
    <sup>4</sup>University of Toronto
    <sup>5</sup>Carnegie Mellon University
    <sup>6</sup>Facebook AI Research
</p>

<div class="w70">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/41/plan2explore-algorithm-diagram.png" alt="">
</div>

<div class="w70">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/41/plan2explore-algorithms.png" alt="">
</div>

**What it says**

Most RL agents are specialists: they excel at one pre-specified task but fail to generalize between tasks. To develop a task-agnostic agent, the authors propose training the agent in an environment without specifying a task. Since a task is defined by the reward function, the agent must learn without any rewards through self-guided exploration. The authors address this exploration problem by learning a world model to estimate "the expected novelty of future situations" without actual environment interactions.

The learning occurs in two phases: the model learning phase and the task adaptation phase. In the first phase, the agent gathers trajectories to train both the world model and the latent disagreement ensemble. The agent is then trained using the world model with latent disagreement rewards to encourage exploration. The trained agent is then deployed to the real environment again to gather more trajectories and the process is repeated. In the second phase, the policy is fine-tuned using the world model with a reward function distilled to the latent space. The trained agent collects task-specific trajectories that are added to the dataset, and the process is repeated.

The Plan2Explore algorithm is tested in the DeepMind control suite in two settings: zero-shot learning and few-shot learning. For zero-shot learning, the agent cannot collect task-specific trajectories in the adaptation phase and must be trained without reward supervision. Plan2Explore achieves better performance than other unsupervised RL agents such as Curiosity and  Model-based Active Exploration (MAX). For few-shot learning, Plan2Explore also achieves better performance than other unsupervised RL agents and is able to achieve scores competitive to RL agent with task supervision (Dreamer) after just a few episodes.

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/2005.05960)
- [GitHub Page](https://ramanans1.github.io/plan2explore/)
- [Poster Talk](https://www.youtube.com/watch?v=GyEzjW1m7kU)
- [Twitter Thread](https://twitter.com/pathak2206/status/1260625950288404480)



------

<div id="more-news"></div>

Here is some more exciting news in RL:

[**Chip Design with Deep Reinforcement Learning**](https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html)
<br/>
The Google Brain team published a preprint and a blog post on using RL to generate chip placements that "are superhuman or comparable on modern accelerator netlists."

[**David Silver: AlphaGo, AlphaZero, and Deep Reinforcement Learning | AI Podcast #86 with Lex Fridman**](https://www.youtube.com/watch?v=uPUEq8d73JI)
<br/>
Lex Friedman's 2-hour conversation with David Silver (DeepMind) is available on YouTube.

[**Introduction to Multi-Armed Bandits**](https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits/)
<br/>
A book on multi-armed bandits by Aleksandrs Slivkins (Microsoft Research) is available online.

[**ML-Agents v1.0**](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/)
<br/>
Unity released version 1.0 of ML-Agents, an open-source project for training agents in games and simulations made with Unity.

[**NetHack Learning Environment**](https://github.com/facebookresearch/nle/)
<br/>
Facebook Research released a new RL environment based on NetHack, a roguelike dungeon exploration game.

[**RL Theory Seminar**](https://sites.google.com/view/rltheoryseminars)
<br/>
Seminars on reinforcement learning theory are being held virtually every Tuesday. The next one is by Nan Jiang (UIUC) on "Information-Theoretic Considerations in Batch Reinforcement Learning."

[**Suphx: Mastering Mahjong with Deep Reinforcement Learning**](https://arxiv.org/abs/2003.13590)
<br/>
Microsoft Research Asia built Suphx, a Mahjong AI trained with deep RL, which achieved rank above most (99.99%) human players.

[**TalkRL: The Reinforcement Learning Podcast**](https://www.talkrl.com/episodes)
<br/>
The TalkRL Podcast has new podcasts with interviews with Danijar Hafner (U. Toronto), Csaba Szepesvari (DeepMind), and Ben Eysenbach (CMU).

<!-- [**TODO**](todo)
<br/>
TODO -->

<!--
## Reject

### Theoretical

- [Q-Learning in Regularized Mean-field Games](https://arxiv.org/abs/2003.12151)
- [Controlling Rayleigh-BÃ©nard convection via Reinforcement Learning](https://arxiv.org/abs/2003.14358)
- [An Information-Theoretic Approach for Path Planning in Agents with Computational Constraints](https://arxiv.org/abs/2005.09611)
- [Making Sense of Reinforcement Learning and Probabilistic Inference](https://openreview.net/forum?id=S1xitgHtvS)
- [Riemannian Proximal Policy Optimization](https://arxiv.org/abs/2005.09195)
- [Mirror Descent Policy Optimization](https://arxiv.org/abs/2005.09814)
- [Provably Efficient Reinforcement Learning with General Value Function Approximation](https://arxiv.org/abs/2005.10804)
- [Information Acquisition Under Resource Limitations in a Noisy Environment](https://arxiv.org/abs/2005.10383)
- [Leverage the Average: an Analysis of Regularization in RL](https://arxiv.org/abs/2003.14089)
- [Novel Policy Seeking with Constrained Optimization](https://arxiv.org/abs/2005.10696)
- [Reinforcement Learning with Feedback Graphs](https://arxiv.org/abs/2005.03789)
- [Synthesizing Safe Policies under Probabilistic Constraints with Reinforcement Learning and Bayesian Model Checking](https://arxiv.org/abs/2005.03898)
- [Reinforcement Learning based Design of Linear Fixed Structure Controllers](https://arxiv.org/abs/2005.04537)
- [Training spiking neural networks using reinforcement learning](https://arxiv.org/abs/2005.05941)
- [Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?](https://arxiv.org/abs/2005.00527)
- [Reinforcement Learning via Fenchel-Rockafellar Duality](https://arxiv.org/abs/2001.01866)
- [On the Global Convergence Rates of Softmax Policy Gradient Methods](https://arxiv.org/abs/2005.06392)
- [A Gradient-Aware Search Algorithm for Constrained Markov Decision Processes](https://arxiv.org/abs/2005.03718)
- [Maximizing Information Gain in Partially Observable Environments via Prediction Reward](https://arxiv.org/abs/2005.04912)
- [A Distributional Analysis of Sampling-Based Reinforcement Learning Algorithms](https://arxiv.org/abs/2003.12239)
- [Safe Reinforcement Learning via Projection on a Safe Set: How to Achieve Optimality?](https://arxiv.org/abs/2004.00915)
- [A Distributional Analysis of Sampling-Based Reinforcement Learning Algorithms](https://arxiv.org/abs/2003.12239)
- [Plannable Approximations to MDP Homomorphisms: Equivariance under Actions](https://arxiv.org/abs/2002.11963)
  - https://twitter.com/ElisevanderPol/status/1234449463508447238
- [Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning](https://arxiv.org/abs/2003.11126)
  - https://ai.googleblog.com/2020/04/off-policy-estimation-for-infinite.html

### Application

- [Batch-Augmented Multi-Agent Reinforcement Learning for Efficient Traffic Signal Optimization](https://arxiv.org/abs/2005.09624)
- [AirRL: A Reinforcement Learning Approach to Urban Air Quality Inference](https://arxiv.org/abs/2003.12205)
- [Towards Cognitive Routing based on Deep Reinforcement Learning](https://arxiv.org/abs/2003.12439)
- [Accelerating Deep Reinforcement Learning With the Aid of a Partial Model: Power-Efficient Predictive Video Streaming](https://arxiv.org/abs/2003.09708)
- [A reinforcement learning based decision support system in textile manufacturing process](https://arxiv.org/abs/2005.09867)
- [Automating Turbulence Modeling by Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2005.09023)
- [Reinforcement Learning for Variable Selection in a Branch and Bound Algorithm](https://arxiv.org/abs/2005.10026)
- [Reinforcement Learning for Caching with Space-Time Popularity Dynamics](https://arxiv.org/abs/2005.09155)
- [Deep Reinforcement Learning for High Level Character Control](https://arxiv.org/abs/2005.10391)
- [Off-policy Learning for Remote Electrical Tilt Optimization](https://arxiv.org/abs/2005.10577)
- [Dampen the Stop-and-Go Traffic with Connected and Automated Vehicles -- A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2005.08245)
- [Lifelong Control of Off-grid Microgrid with Model Based Reinforcement Learning](https://arxiv.org/abs/2005.08006)
- [Is Deep Reinforcement Learning Ready for Practical Applications in Healthcare? A Sensitivity Analysis of Duel-DDQN for Sepsis Treatment](https://arxiv.org/abs/2005.04301)
- [Intelligent Roundabout Insertion using Deep Reinforcement Learning](https://arxiv.org/abs/2001.00786)
- [Reinforcement Learning for UAV Autonomous Navigation, Mapping and Target Detection](https://arxiv.org/abs/2005.05057)
- [Mobile Robot Path Planning in Dynamic Environments through Globally Guided Reinforcement Learning](https://arxiv.org/abs/2005.05420)
- [Reinforcement Learning for Thermostatically Controlled Loads Control using Modelica and Python](https://arxiv.org/abs/2005.04444)
- [I love your chain mail! Making knights smile in a fantasy game world: Open-domain goal-oriented dialogue agents](https://arxiv.org/abs/2002.02878)
- [Deep Reinforcement Learning for Organ Localization in CT](https://arxiv.org/abs/2005.04974)
- [From Simulation to Real World Maneuver Execution using Deep Reinforcement Learning](https://arxiv.org/abs/2005.07023)
- [Delay-aware Resource Allocation in Fog-assisted IoT Networks Through Reinforcement Learning](https://arxiv.org/abs/2005.04097)
- [Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding Behaviors using Deep Reinforcement Learning](https://arxiv.org/abs/2005.09476)
- [A Deep Q-learning/genetic Algorithms Based Novel Methodology For Optimizing Covid-19 Pandemic Government Actions](https://arxiv.org/abs/2005.07656)
- [Learning medical triage from clinicians using Deep Q-Learning](https://arxiv.org/abs/2003.12828)
- [Learning to Ask Medical Questions using Reinforcement Learning](https://arxiv.org/abs/2004.00994)
- [Qd-tree: Learning Data Layouts for Big Data Analytics](https://arxiv.org/abs/2004.10898)
- [Towards Automated Safety Coverage and Testing for Autonomous Vehicles with Reinforcement Learning](https://arxiv.org/abs/2005.13976)
- [Reinforcement Learning with Iterative Reasoning for Merging in Dense Traffic](https://arxiv.org/abs/2005.11895)
- [Intelligent Residential Energy Management System using Deep Reinforcement Learning](https://arxiv.org/abs/2005.14259)
- [Fast Risk Assessment for Autonomous Vehicles Using Learned Models of Agent Futures](https://arxiv.org/abs/2005.13458)
- [Generator and Critic: A Deep Reinforcement Learning Approach for Slate Re-ranking in E-commerce](https://arxiv.org/abs/2005.12206)
- [Meta-Reinforcement Learning for Trajectory Design in Wireless UAV Networks](https://arxiv.org/abs/2005.12394)
- [Model-free Reinforcement Learning for Stochastic Stackelberg Security Games](https://arxiv.org/abs/2005.11853)
- [Optimization-driven Deep Reinforcement Learning for Robust Beamforming in IRS-assisted Wireless Communications](https://arxiv.org/abs/2005.11885)
- [Integrating LEO Satellite and UAV Relaying via Reinforcement Learning for Non-Terrestrial Networks](https://arxiv.org/abs/2005.12521)
- [Q-NAV: NAV Setting Method based on Reinforcement Learning in Underwater Wireless Networks](https://arxiv.org/abs/2005.13521)


### Robotics

- [Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics](https://arxiv.org/abs/2005.05719)
- [Learning and Reasoning for Robot Dialog and Navigation Tasks](https://arxiv.org/abs/2005.09833)
- [Deep Reinforcement learning for real autonomous mobile robot navigation in indoor environments](https://arxiv.org/abs/2005.13857)

### NLP

- [Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation](https://arxiv.org/abs/2005.04379)
- [Reinforced Rewards Framework for Text Style Transfer](https://arxiv.org/abs/2005.05256)


### Multi-agent

- [Information State Embedding in Partially Observable Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2004.01098)
- [Delay-Aware Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2005.05441)
- [Parallel Knowledge Transfer in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2003.13085)
- [Learning to cooperate: Emergent communication in multi-agent navigation](https://arxiv.org/abs/2004.01097)
- [Multi-Agent Reinforcement Learning for Problems with Combined Individual and Team Reward](https://arxiv.org/abs/2003.10598)


### Survey

- [From Chess and Atari to StarCraft and Beyond: How Game AI is Driving the World of AI](https://arxiv.org/abs/2002.10433)
- [Automatic Curriculum Learning For Deep RL: A Short Survey](https://arxiv.org/abs/2003.04664)
- [Explainable Reinforcement Learning: A Survey](https://arxiv.org/abs/2005.06247)
- [A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments](https://arxiv.org/abs/2005.10619)

### Misc.

- [TOMA: Topological Map Abstraction for Reinforcement Learning](https://arxiv.org/abs/2005.06061)
- [Adaptive Conditional Neural Movement Primitives via Representation Sharing Between Supervised and Reinforcement Learning](https://arxiv.org/abs/2003.11334)
- [Giving Up Control: Neurons as Reinforcement Learning Agents](https://arxiv.org/abs/2003.11642)
- [Deep Reinforcement Learning with Smooth Policy](https://arxiv.org/abs/2003.09534)
- [spinning-up-basic](https://github.com/Kaixhin/spinning-up-basic)
- [Obstacle Tower Without Human Demonstrations: How Far a Deep Feed-Forward Network Goes with Reinforcement Learning](https://arxiv.org/abs/2004.00567)
- [Scott Niekum's Talk on "Scaling Probabilistically Safe Learning to Robotics"](https://www.youtube.com/watch?v=slGmQv_pWs0)
- [Multi-agent learning & evaluation for open world games - Sam Devlin, Microsoft Research](https://www.youtube.com/watch?v=eYosANC7yeQ)
- [DQN and DRQN in partially observable gridworlds](https://kam.al/blog/drqn/)
- [REPLAB: A Reproducible Low-Cost Arm Benchmark Platform For Robotic Learning](https://sites.google.com/view/replab/)
- [Continual Reinforcement Learning with Multi-Timescale Replay](https://arxiv.org/abs/2004.07530)
- [An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning](https://arxiv.org/abs/2005.04646)
- [Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement Learning Problems](https://arxiv.org/abs/2005.04536)
- [Probabilistic Guarantees for Safe Deep Reinforcement Learning](https://arxiv.org/abs/2005.07073)
- [Proxy Experience Replay: Federated Distillation for Distributed Reinforcement Learning](https://arxiv.org/abs/2005.06105)
- [Stealthy and Efficient Adversarial Attacks against Deep Reinforcement Learning](https://arxiv.org/abs/2005.07099)
- [Triple-GAIL: A Multi-Modal Imitation Learning Framework with Generative Adversarial Nets](https://arxiv.org/abs/2005.10622)
- [Experience Augmentation: Boosting and Accelerating Off-Policy Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2005.09453)
- [Causal Feature Learning for Utility-Maximizing Agents](https://arxiv.org/abs/2005.08792)
- [Optimizing for the Future in Non-Stationary MDPs](https://arxiv.org/abs/2005.08158)
- [Unbiased Deep Reinforcement Learning: A General Training Framework for Existing and Future Algorithms](https://arxiv.org/abs/2005.07782)
- [Unified Models of Human Behavioral Agents in Bandits, Contextual Bandits and RL](https://arxiv.org/abs/2005.04544)
- [Model-Augmented Actor-Critic: Backpropagating through Paths](https://arxiv.org/abs/2005.08068)
- [Concept Learning in Deep Reinforcement Learning](https://arxiv.org/abs/2005.07870)
- [Local and Global Explanations of Agent Behavior: Integrating Strategy Summaries with Saliency Maps](https://arxiv.org/abs/2005.08874)
- [Delay-Aware Model-Based Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2005.05440)
- [Policy Gradient Class](https://www.youtube.com/watch?v=_RQYWSvMyyc)
- [Augmented Q Imitation Learning (AQIL)](https://arxiv.org/abs/2004.00993)
- [A macro agent and its actions](https://arxiv.org/abs/2004.00058)
- [Do recent advancements in model-based deep reinforcement learning really improve data efficiency?](https://arxiv.org/abs/2003.10181)
- [Learning Sparse Rewarded Tasks from Sub-Optimal Demonstrations](https://arxiv.org/abs/2004.00530)
- [Constrained-Space Optimization and Reinforcement Learning for Complex Tasks](https://arxiv.org/abs/2004.00716)
- [Distributional Reinforcement Learning with Ensembles](https://arxiv.org/abs/2003.10903)
- [Deep Sets for Generalization in RL](https://arxiv.org/abs/2003.09443)
- [Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning](https://arxiv.org/abs/2005.06800)
- [Exploration of Reinforcement Learning for Event Camera using Car-like Robots](https://arxiv.org/abs/2004.00801)
- [Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning](https://arxiv.org/abs/2003.12909)
- [Adaptive Reward-Poisoning Attacks against Reinforcement Learning](https://arxiv.org/abs/2003.12613)
- [Off-policy Policy Evaluation For Sequential Decisions Under Unobserved Confounding](https://arxiv.org/abs/2003.05623)
- [Optimistic Exploration even with a Pessimistic Initialisation](https://arxiv.org/abs/2002.12174)
- [AMRL: Aggregated Memory For Reinforcement Learning](https://www.microsoft.com/en-us/research/publication/amrl-aggregated-memory-for-reinforcement-learning/)
- [Entity Abstraction in Visual Model-Based Reinforcement Learning](https://sites.google.com/view/op3website/)
- [Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions](https://arxiv.org/abs/2002.03478)
  - https://twitter.com/JFutoma/status/1229845958550118400
- [A Game Theoretic Framework for Model Based Reinforcement Learning](https://arxiv.org/abs/2004.07804)
  - https://sites.google.com/view/mbrl-game
  - https://twitter.com/Vikashplus/status/1251185709328723970
- [Weakly-Supervised Reinforcement Learning for Controllable Behavior](https://arxiv.org/abs/2004.02860)
  - https://twitter.com/chelseabfinn/status/1247608453759025157
- [AAMAS2020: Unsupervised Reinforcement Learning](https://www.youtube.com/watch?v=4vK6X9Jrncs)
- [AAMAS2020: Gifting in Multi-Agent Reinforcement Learning](https://underline.io/lecture/209-gifting-in-multi-agent-reinforcement-learning)
- [Interference and Generalization in Temporal Difference Learning](https://arxiv.org/abs/2003.06350)


### Too Old

- [Reward-Conditioned Policies](https://arxiv.org/abs/1912.13465)
- [Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603)
- [Training language GANs from Scratch](https://arxiv.org/abs/1905.09922)
- [The Journey is the Reward: Unsupervised Learning of Influential Trajectories](https://arxiv.org/abs/1905.09334)
- [DeepMDP: Learning Continuous Latent Space Models for Representation Learning](https://arxiv.org/abs/1906.02736)
- [A Survey of Reinforcement Learning Informed by Natural Language](https://arxiv.org/abs/1906.03926)
- https://arxiv.org/abs/1910.08210
- [A View on Deep Reinforcement Learning in System Optimization](https://arxiv.org/abs/1908.01275)
- [Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology](https://arxiv.org/abs/1905.12767)
- [Growing Action Spaces](https://arxiv.org/abs/1906.12266)
- [SQIL](https://arxiv.org/abs/1905.11108)
  - https://twitter.com/svlevine/status/1254530587311828993
- [A Closer Look at Deep Policy Gradients](http://gradientscience.org/policy_gradients_pt3)
- [Improving Deep Neuroevolution via Deep Innovation Protection](https://arxiv.org/abs/2001.01683)
- [Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2003.08839)
- [A Distributional View on Multi-Objective Policy Optimization](https://arxiv.org/abs/2005.07513)
- [MDPs with Unawareness in Robotics](https://arxiv.org/abs/2005.10381)

### Others (Non-RL)

- [Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)](https://arxiv.org/abs/2003.12206)
- https://arxiv.org/abs/1803.00942
- https://arxiv.org/abs/2002.10365
- https://arxiv.org/abs/2003.02139
- https://arxiv.org/abs/2003.01897
- https://arxiv.org/abs/2002.06776
- https://arxiv.org/abs/2002.11770
- https://arxiv.org/abs/2003.00394
- https://arxiv.org/abs/2002.11448
- https://arxiv.org/abs/1811.06032
- https://arxiv.org/abs/2003.02237
- https://arxiv.org/abs/2002.11803
- https://arxiv.org/abs/2002.11328
- https://arxiv.org/abs/2003.01054
- https://arxiv.org/abs/2003.00179
- https://arxiv.org/abs/2002.11080
- https://arxiv.org/abs/2002.06668
- https://arxiv.org/abs/2002.10544
- https://arxiv.org/abs/2002.10964
- https://arxiv.org/abs/2002.06262
- https://arxiv.org/abs/2002.10376
  -->
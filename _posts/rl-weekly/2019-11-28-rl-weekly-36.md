---
layout: post
title: "RL Weekly 36: AlphaZero with a Learned Model achieves SotA in Atari"
author: Seungjae Ryan Lee
permalink: /rl-weekly/36
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/36/muzero.png
image_type: contain
excerpt: "In this issue, we look at MuZero, DeepMind's new algorithm that learns a model and achieves AlphaZero performance in Chess, Shogi, and Go and achieves state-of-the-art performance on Atari. We also look at Safety Gym, OpenAI's new environment suite for safe RL."

nav:
- name: "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"
  permalink: "#muzero"
- name: "Safety Gym"
  permalink: "#safety-gym"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 35: Escaping Local Optimas in Distance-based Rewards and Choosing the Best Teacher"
  link: /rl-weekly/35
  image: /assets/blog/rl-weekly/35/sibling_rivalry.png
  image_type: contain
---


{% include revue.html %}

<style>
.letter, .letter p {
  color: gray;
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
.letter a {
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
</style>

<div class="letter">
<p>
Dear readers,
</p>
<p>
We have reached 800 subscribers! Thank you so much for subscribing to RL Weekly and sharing it with your friends. I will try my best to provide you interesting news in reinforcement learning. Anyways, here is the 36th issue!
</p>
<p>
As always, please feel free to <a href="mailto:seungjaeryanlee@gmail.com">email me</a> or <a href="https://forms.gle/yZiHUXbtph8msVHn9">leave any feedback</a>. Your input is always appreciated.
</p>
<p>
- Ryan
</p>
</div>



## Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model {#muzero}

<p class="authors" style="font-size: 1em">
Julian Schrittwieser<sup>1*</sup>,
Ioannis Antonoglou<sup>12*</sup>,
Thomas Hubert<sup>1*</sup>,
Karen Simonyan<sup>1</sup>,
Laurent Sifre<sup>1</sup>,
Simon Schmitt<sup>1</sup>,
Arthur Guez<sup>1</sup>,
Edward Lockhart<sup>1</sup>,
Demis Hassabis<sup>1</sup>,
Thore Graepel<sup>12</sup>,
Timothy Lillicrap<sup>1</sup>,
David Silver<sup>12*</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>DeepMind
    <sup>2</sup>University College London
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/36/muzero.png" alt="">
</div>

**What it says**

RL can be divided into Model-Based RL (MBRL) and Model-Free RL (MFRL). Model-based RL use an environment model for planning, whereas model-free RL learn the optimal policy directly from interactions. Model-based RL has achieved superhuman level of performance in Chess, Go, and Shogi, where the model is given and the game requires sophisticated lookahead. However, model-free RL performs better in environments with high-dimensional observations where the model must be learned. Model-based RL suffer from compounded error from jointly learning representation and model and planning.

The representation function receives past observations from a selected trajectory as input and returns an initial hidden state. The dynamics function of MuZero is a recurrent process, where given the previous hidden state and a hypothetical action, it returns the predicted reward and the next hidden state. This hidden state is used by the prediction network to predict the value function and the policy, similar to AlphaZero. A generalized verison of MCTS is used for planning and action selection. The represntation function, dynamics function, and the prediction function are all trained jointly by backpropagation through time (Appendix G).

Unlike AlphaZero, MuZero uses its learned dynamics model without given knowledge of the game. The algorithm searches through invalid actions as well as valid actions, and does not discern terminal node (Appendix A). Yet, MuZero matches or exceeds the performance of AlphaZero in Chess, Shogi, and Go, despite not being given a model. For each board game, MuZero was trained for 12 hours on 16 TPUs for training and 1000 TPUs for self-play (Appendix G).

MuZero also sets a new state-of-the-art performance in Atari games. With 20 billion frames, it exceeds the performance of R2D2 trained with 37.5 billion frames. Its sample-efficient variant named "MuZero Reanalyze" that reanalyze old trajectories also achieve performance better than IMPALA, Rainbow, UNREAL, and LASER while only using 200 million frames to match the number of frames used (Appendix H). The training took 12 hours with 8 TPUs for training and 32 hours for self-play (Appendix G).

**Read more**

- [arXiv Preprint](https://arxiv.org/abs/1911.08265)

**External resources**

- [Value Prediction Network (arXiv Preprint)](https://arxiv.org/abs/1707.03497)
- [r/machinelearning Subreddit Post](https://www.reddit.com/r/MachineLearning/comments/dzakrs/r_191108265_mastering_atari_go_chess_and_shogi_by/)
- [r/reinforcementlearning Subreddit Post](https://www.reddit.com/r/reinforcementlearning/comments/dzaui6/muzero_mastering_atari_go_chess_and_shogi_by/)


## Safety Gym {#safety-gym}

<p class="authors" style="font-size: 1em">
Alex Ray<sup>1*</sup>,
Joshua Achiam<sup>1*</sup>,
Dario Amodei<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>OpenAI
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/36/safety_gym_doggo.png" alt="">
</div>

**What it says**

In environments outside of simulators, the behavior of RL agents could have substantial consequences. Yet, as RL agents require trial and error to learn, the "safe exploration" problem naturally follows. The authors define safe RL as "constrained RL." In constrained RL, every policy is assigned a constrained cost, and the optimal policy must have a constrained cost below some threshold. The validity of this approach can be inferred from its similarity to cumulative reward and the reward hypothesis that state that a scalar reward can describe the goal of an agent.

The authors also provide Safety Gym, a set of 18 environments that vary in difficulty. They are robot navigation environments, with 3 types of robots, 3 different tasks, and 2 difficulty settings. The authors test several baselines such as TRPO, PPO, their Lagrangian penalized versions, and Constrained Policy Optimization (CPO).

**Read more**

- [OpenAI Blog Post](https://openai.com/blog/safety-gym/)
- [Paper](https://cdn.openai.com/safexp-short.pdf)
- [GitHub Repository: Safety Gym](https://github.com/openai/safety-gym)
- [GitHub Repository: Starter Agents](https://github.com/openai/safety-starter-agents)








------

<div id="more-news"></div>

Here are some more exciting news in RL:

[**IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks**](https://clvrai.github.io/furniture/)
<br/>
The IKEA Furniture Assembly Environment is an environment that simulates furniture assembly of a Baxter and Sawyer robot.

[**Data-efficient Co-Adaptation of Morphology and Behaviour with Deep Reinforcement Learning**](https://arxiv.org/abs/1911.06832)
<br/>
To imitate how humans and animals evolved, we co-adapt robot morphology and its controller in a data-efficient manner.

[**Voyage Deepdrive**](https://deepdrive.voyage.auto/)
<br/>
Deepdrive is an self-driving car simulator that supports reinforcement learning agents.

[**Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller**](https://arxiv.org/abs/1911.09676)
<br/>
Learn by looking at a demonstration from a third-person perspective through a hierarchical structure of high-level goal generator and low-level controller.

<!-- [**TODO**](todo)
<br/>
TODO -->




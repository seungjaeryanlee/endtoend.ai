---
layout: post
title: "DLND: 6. Training Neural Network 2"
permalink: /mooc/dlnd/6
---

In the [last lesson](/mooc/dlnd/5), we learned about overfitting and underfitting, and few types of regularization techniques. There are many other choices to make when training a neural network well. In this lesson, we will discuss a few important practical tricks for training a neural network.

### Vanishing Gradient

One problem of the sigmoid activation function is **Vanishing Gradient**. As the name suggests, it is the problem of gradient "vanishing", or becoming too small that it hinders learning. Because of the shape of the sigmoid function, if $\lvert x \rvert >> 0$, then $\sigma'(x) \simeq 0$.

![Vanishing Gradient Problem]({{ "assets/DLND-6/vanishing_gradient.png" | absolute_url }})

This problem is compounded by the fact that we are using a deep neural network. The sigmoid activation function is used in every perceptron in every layer, so when we compute the gradient of the weights with the chain rule, we take a product of small values and get a tiny value.

### Other Activation Functions

To avoid this problem, we use other activation functions. There are several widely used activation functions. One of them is the **Hyperbolic tangent function** $\tanh(x)$.

![Hyperbolic tangent function]({{ "assets/DLND-6/tanh.png" | absolute_url }})

This function is a scaled version of the sigmoid function, so it has similar problems for big $x$, but because it ranges from $-1$ to $1$, it is better than the sigmoid function.

An even popular activation function is the **Rectified Linear Unit (ReLU) function**. Despite the complicated name, the function is very simple: ReLU is a piecewise function with $y=x$ if $x$ is positive, and $y=0$ otherwise. It is also written as $\max(x, 0)$.

![ReLU]({{ "assets/DLND-6/relu.png" | absolute_url }})

The great advantage of ReLU is that its derivative is $1$ when $x$ is positive.

![Example of activation functions used]({{ "assets/DLND-6/activation_example.png" | absolute_url }})

Thus, it is common to use ReLU activation function everywhere except the output layer. In the output layer, the activation function is dependent on the type of the problem. If we have a classification problem, the output should be in $[0, 1]$, so we need to use the sigmoid function. If we have a regression problem, the output should be in $[0, \infty)$, so we use the ReLU function (assuming no negative values).

### Batch vs. Stochastic Gradient Descent

Until now, we learned about **Batch Gradient Descent**. By "batch", we mean that for each step of feed-forward and backpropagation, we use all the data in the dataset to calculate the error. This can be very slow or even impossible if the training dataset is huge.

Thus, we also use **Stochastic Gradient Descent**, a method of using only one data point for each step of gradient descent. Because we only use one data point instead of the entire dataset, the steps are less accurate than the steps in batch gradient descent. However, with enough steps, it still reaches the minimum, and in many cases, it reaches the minimum faster than the batch gradient descent method.

A balance between the batch gradient descent and stochastic gradient descent is to use small mini-batches of the entire dataset in each step. This is called **Mini-batch Gradient Descent**. Mini-batch gradient descent is very popular because it has the advantages of both batch and stochastic gradient descent. It can take a step quickly, and also use vectorization to accelerate feed-forward and backpropagation.

###Local Minima

Another problem of gradient descent is that it is susceptible to converging to local minima. Once we reach a local minimum, the gradient is close to zero, so the model cannot find the global minimum.

![Local Minima Problem]({{ "assets/DLND-6/local_minima.png" | absolute_url }})

### Random Restart

One way to solve this problem is to train the model multiple times, initializing weights differently each time. This is called the Random Restart technique.  Intuitively, this increases the probability of finding the global minimum.

![Random Restart]({{ "assets/DLND-6/random_restart.png" | absolute_url }})

### Momentum

Another solution for the local minima problem is to implement **Momentum**. If gradient descent has momentum, it can pass through small humps, which makes the model more likely to reach global minimum.

We calculate the momentum $\beta$ by looking at previous steps and averaging them. We use a weighted average so that recent steps matter more than steps taken long time ago.

$$
\theta := \theta - \alpha \nabla_\theta J + \beta \Delta \theta
$$

### Learning Rate Decay

We talked about the learning rate $\alpha$ and how important it is for gradient descent. Learning rate too large can make gradient descent oscillate or diverge and miss the minimum, while learning rate too small can make gradient descent take too long to converge to the minimum.

One commonly used trick is to decay the learning rate as we train the model. This is called **Learning Rate Decay**. With learning rate decay, we start with huge steps that can quickly reach the vicinity of the minimum, and gradually take smaller steps to converge to the minimum.


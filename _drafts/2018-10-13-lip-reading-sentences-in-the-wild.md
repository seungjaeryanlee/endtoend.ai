---
layout: post
title: "Slow Papers: Lip Reading Sentences in the Wild (Chung et al., 2016)"
author: Seungjae Ryan Lee
permalink: /slowpapers/lip-reading-sentences-in-the-wild/

front_image: /assets/blog/slowpapers/lip-reading-sentences-in-the-wild/front.png
meta_image: /assets/blog/slowpapers/lip-reading-sentences-in-the-wild/front.png
front_image_type: contain
front_text: >
    The goal of this work is to recognise phrases and sentences beign spoken by a talking face, with or without the audio. Unlike previous works that have focused on recognising a limited number of words and phrases, we tackle lip reading as an open world problem - unconstrained natural language sentences, and in the wild videos.
excerpt: >
    The goal of this work is to recognise phrases and sentences beign spoken by a talking face, with or without the audio. Unlike previous works that have focused on recognising a limited number of words and phrases, we tackle lip reading as an open world problem - unconstrained natural language sentences, and in the wild videos.

nav:
- name: 1 Introduction
  permalink: '#1-introduction'
- name: 2 Architecture
  permalink: '#2-architecture'
- name: 3 Training strategy
  permalink: '#3-training-strategy'
- name: 4 Dataset
  permalink: '#4-dataset'
- name: 5 Experiments
  permalink: '#5-experiments'
- name: 6 Summary and extensions
  permalink: '#6-summary-and-extensions'
- name: Final Thoughts
  permalink: '#final-thoughts'
---

![Abstract]({{ "/assets/blog/slowpapers/lip-reading-sentences-in-the-wild/front.png" | absolute_url }})

**Title**: Lip Reading Sentences in the Wild

**Authors**
<div>
<ul class="slowpapers__authors">
  <li>Joon Son Chung</li>
  <li>Andrew Senior</li>
  <li>Oriol Vinyals</li>
  <li>Andrew Zisserman</li>
</ul>
</div>

**Prerequisites**
 - TBA

**Accompanying Resources**
 - TBA

<hr/>

This is a part of the [**Slow Papers**](/slowpapers) series that peruses each selected paper slowly to gain a deeper understanding of the paper.

<hr/>



## 1 Introduction
### 1.1 Related Works
## 2 Architecture
### 2.1 Watch: Image encoder
### 2.2 Listen: Audio encoder
### 2.3 Spell: Character decoder
## 3 Training strategy
### 3.1 Curriculum learning
### 3.2 Scheduled sampling
### 3.3 Multi-modal training
### 3.4 Training with noisy audio
### 3.5 Implementation details
## 4 Dataset
### 4.1 Audio-only data
## 5 Experiments
### 5.1 Evaluation
### 5.2 Human experiment
### 5.3 LRW dataset
### 5.4 GRID dataset
## 6 Summary and extensions


<hr/>



## Final Thoughts

**Questions**
 - TBA

**Recommended Next Papers**
 - TBA

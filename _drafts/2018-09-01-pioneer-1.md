---
layout: post
title: "Pioneer Day 1: Q Learning on FrozenLake"
author: Seungjae Ryan Lee
permalink: /blog/pioneer-1/
published: false

front_image: /assets/blog/pioneer/1/front.jpg
excerpt: >
    Today was the first day of the four-week Pioneer tournament, where I
    will demystify the efffects of experience replay. I implemented a tabular
    Q-Learning Agent on FrozenLake to compare with a version with experienc
    replay.
---

*This is part of the post series for the **Pioneer Tournament**, where
participants work on their own projects for 30 days and get feedback from
other participants. You can read more about my project, **Demystifying
Experience Replay** [here](/pioneer). My source code is publicly available on
[GitHub](https://github.com/seungjaeryanlee/pioneer).*

## Weeks

- [Week 1: Q-Learning on FrozenLake](/blog/pioneer-1)

## Q-Learning

In reinforcement learning, one of the most important value to estimate is that state-action value (also known as the $Q$ value) given a policy. This value $Q_\pi(s, a)$ is the expected total reward the agent will receive following a policy $\pi$ after taking action $a$ on state $s$. If the agent knows the true state-action value for all states and actions, at each state $s$, it can greedily choose the action with the highest $Q$ value, it can find a better policy $\pi'$. Thoroughly repeated policy evaluation (finding the $Q$ values) and policy improvement (finding better policy $\pi'$), the agent can find an optimal policy. This procedure is known as the *Generalized Policy Iteration*, and is the core of most reinforcement learning algorithms.

Q-Learning is one of the fundamental reinforcement learning algorithm used to estimate the state-action value of a policy. Deep Q-Networks (DQNs), Q-Learning with neural networks, have shown amazing results in *Atari 2600* games and marked the triggered the study of Deep Reinforcement Learning in 2013. In a few days, we will talk about DQNs, but today, we focus on the easy, tabular case, where the environment is small enough so that state-action values for each state and action can be stored in a table.

Q-Learning can be effectively summarized with a single equation:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha (R + \max_{a'} Q(s', a') - Q(s, a))
$$


## FrozenLake

Reinforcement Learning is still a very young field, and thus there are very few standard libraries. Fortunately, there is a standard library for environments called the **Gym** that was created by OpenAI. Gym set a standard interface for environment so that agents can be designed to be compatible with numerous different environments with a significant change. Gym also has some excellent environments to test the agent. One of them is `FrozenLake`, an environment where the agent wants to reach a goal without falling into a hole.

## What's Next?
---
layout: page
title: "AIND: 16. Bayes Network"
permalink: /mooc/aind/16/
---

In this lesson, we will learn about Bayes network: an efficient representation of relationships between random variables (events).

*Note: Both AIND and AIMA had very little explanation of D-Separation, so I have omitted it from the notes.*

### Bayes Network

In general, to specifying joint probability distribution of $n$ random variables, we need  $2^n$ probability distributions. This is impractical for high $n$. Fortunately, by using Bayes network, we often can drastically reduce the number. The **Bayes network** is a directed graph that can concisely represent the joint probability distribution $P(X_1, X_2, \ldots, X_n)$ of many random variables $X_1, X_2, \ldots, X_n$. 

Here is a simple example of a Bayes network:

![An example of a Bayes network]({{ "assets/mooc/aind/16/bayes_network_ex.png" | absolute_url }})

Suppose that $A$ is cancer and $B$ is a test for cancer. By knowing $P(A), P(B \mid A), P(B \mid \lnot A)$, we can compute any joint probability of $A$ and $B$ through negation and Bayes' Theorem.

![Another example of Bayes Network]({{ "assets/mooc/aind/16/bayes_network_ex2.png" | absolute_url }})

Now let's consider a Bayes network with two tests. Knowing the probabilities $P(C), P(T_1 \mid C), P(T_1 \mid \lnot C), P(T_2 \mid C), P(T_ \mid \lnot C)$, we can calculate all joint probabilities of $C, T_1, T_2$. For example, we can calculate  $P(C \mid T_1 = +, T_2 = +)$ :

$$ P(C \mid ++) = \frac{P(++ \mid C) P(C)}{P(P(++ \mid C)P(C) + P(++ \mid \lnot C)P(\lnot C))} \simeq 0.1698$$

Note that we assumed the fact that $T_1$ and $T_2$ are conditionally independent: $P(T_1 \mid C, T_2) = P(T_1 \mid C)$. This is an important **independence assumption**: in a Bayes network, each random variable is conditionally independent of its non-descendants given its parents.

We can also calculate $P(T_2 = + \mid T_1 = +)$ with total probability:

$$P(+_2 \mid +_1) = P(+_2 \mid +_1, C) P(C \mid +_1) + P(+_2 \mid +_1, \lnot C) P(\lnot C \mid +_1) \simeq 0.230$$

### Conditional Probability Distribution

In the Bayes network with two random variables $A$ and $B$, we needed to specify three probabilities: $P(A), P(B \mid A), P(B \mid \lnot A)$. In the Bayes network with three random variables $C, T_1, T_2$, we needed to specify five probabilities: $P(C), P(T_1 \mid C), P(T_1 \mid \lnot C), P(T_2 \mid C), P(T_2 \mid \lnot C)$. With these probabilities, it is possible to calculate any probability in this network. These probabilities are called the **conditional probability distribution**.

We can count the number of conditional probability distribution for a node by checking its parents. If a node $A$ has $k$ parents, it needs $2^k$ probabilities to specify the probability of $A$ given each combination of its $k$ parents $P(A \mid P_1, \ldots P_k)$.

![Yet another example of a Bayes Network]({{ "assets/mooc/aind/16/bayes_network_ex3.png" | absolute_url }})

For example, in the Bayes network above, $A$, $B$, $C$ have $0$ parents, so it each needs 1 conditional probability distribution. $D$ has $3$ parents so it needs $8$, $G$ has $2$ parents so it needs $4$, and $E, F$ has $1$ parent so it needs $2$ conditional probability distributions. Therefore, in total, we need to specify $19$ conditional probability distributions to fully specify this Bayes network.

$N = N_A + N_B + N_C + N_D + N_E + N_F + N_G = 1+1+1+8+2+2+4=19$

